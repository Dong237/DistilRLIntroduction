
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 7. Planning and Learning with Tabular Methods &#8212; DistilRLIntro 0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=e1a75a79"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Contents/7_planning_learning_acting';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 8. On-policy Prediction with Approximation" href="8_on_policy_prediction_with_approximation.html" />
    <link rel="prev" title="Chapter 6. Temporal-Difference Learning" href="6_temporal_difference_learning.html" /> 
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>   
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="DistilRLIntro 0.1 documentation - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="DistilRLIntro 0.1 documentation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">I. Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="0_prelude.html">Chapter 0. Prelude</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_intro.html">Chapter 1. Introduction to RL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">II.1. Fundamentals of Reinforcement Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2_multi_armed_bandits.html">Chapter 2. Multi-armed Bandit</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_markov_decision_process.html">Chapter 3. Finite Markov Decision Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_dynamic_programming.html">Chapter 4. Dynamic Programming</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">II.2. Sample-based Learning Methods</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="5_monte_carlo_methods.html">Chapter 5. Monte Carlo Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="6_temporal_difference_learning.html">Chapter 6. Temporal-Difference Learning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 7. Planning and Learning with Tabular Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">III.1. Value Function Approximation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="8_on_policy_prediction_with_approximation.html">Chapter 8. On-policy Prediction with Approximation</a></li>
<li class="toctree-l1"><a class="reference internal" href="9_on_policy_control_with_approximation.html">Chapter 9. On-policy Control with Approximation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">III.2. Policy Approximation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_policy_gradient_methods.html">Chapter 10. Policy Gradient Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_modern_policy_gradient_methods.html">Chapter 11. Modern Policy Gradient Methods</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/Dong237/DistilRLIntroduction" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Dong237/DistilRLIntroduction/blob/master/Contents/7_planning_learning_acting.md?plain=1" target="_blank"
   class="btn btn-sm btn-source-file-button dropdown-item"
   title="Show source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">Show source</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Dong237/DistilRLIntroduction/edit/master/Contents/7_planning_learning_acting.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Dong237/DistilRLIntroduction/issues/new?title=Issue%20on%20page%20%2FContents/7_planning_learning_acting.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Contents/7_planning_learning_acting.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 7. Planning and Learning with Tabular Methods</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#models-and-planning">7.1 Models and Planning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dyna-integrated-planning-acting-and-learning">7.2 Dyna: Integrated Planning, Acting, and Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-the-model-is-wrong">7.3 When the model is wrong</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">7.4 Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                   <section class="tex2jax_ignore mathjax_ignore" id="chapter-7-planning-and-learning-with-tabular-methods">
<h1>Chapter 7. Planning and Learning with Tabular Methods<a class="headerlink" href="#chapter-7-planning-and-learning-with-tabular-methods" title="Link to this heading">#</a></h1>
<p>In this chapter we develop a unified view of reinforcement learning methods that require a model of the environment (<span class="math notranslate nohighlight">\(\textit{model-based}\)</span> RL methods), such as dynamic programming, and methods that can be used without a model (<span class="math notranslate nohighlight">\(\textit{model-free}\)</span> RL methods), such as Monte Carlo and temporal-difference methods.</p>
<p><strong>Model-based methods rely on <span class="math notranslate nohighlight">\(\textit{planning}\)</span></strong> as their primary component, while <strong>model-free methods primarily rely on <span class="math notranslate nohighlight">\(\textit{learning}\)</span></strong>. Although there are real differences between these two kinds of methods, there are also great similarities:</p>
<ul class="simple">
<li><p>The heart of both kinds of methods is the computation of value functions.</p></li>
<li><p>All the methods are based on looking ahead to future events, computing a backed-up value, and then using it as an update target for an approximate value function.</p></li>
</ul>
<p>In addition to the unified view of planning and learning methods, a second theme in this chapter is the benefits of planning in small, incremental steps. This enables planning to be interrupted or redirected at any time with little wasted computation, which appears to be a key requirement for efficiently intermixing planning with acting and with learning of the model.</p>
<section id="models-and-planning">
<h2>7.1 Models and Planning<a class="headerlink" href="#models-and-planning" title="Link to this heading">#</a></h2>
<ul>
<li><p><strong><span class="math notranslate nohighlight">\(\textit{Model}\)</span> (of the environment)</strong>: anything that an agent can use to predict how the environment will respond to its actions,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\textit{Distribution models}\)</span>: produce a description of all possibilities and their probabilities. E.g., the model used in DP (i.e., the dynamics of the environment: <span class="math notranslate nohighlight">\(p(r, s' | s, a)\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\textit{Sample models}\)</span>: produce just one of the possibilities, sampled according to the probabilities.</p></li>
<li><p>Both kinds of models are used to <span class="math notranslate nohighlight">\(\textit{simulate}\)</span> the environment and produce <span class="math notranslate nohighlight">\(\textit{simulated experience}\)</span>. Given a starting state and a policy, a sample model could produce an entire episode, and a distribution model could generate all possible episodes and their probabilities.</p></li>
</ul>
</li>
<li><p><strong><span class="math notranslate nohighlight">\(\textit{Planning}\)</span></strong>: any computational process that takes a <span class="math notranslate nohighlight">\(\textit{model}\)</span> as input and produces or improves a policy for interacting with the modeled environment, illustrated as:</p>
<div class="math notranslate nohighlight">
\[
		model \xrightarrow{\text{planning}} policy
	\]</div>
<p>there are two distinct approaches to planning:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\textit{state-space planning}\)</span>: a search through the state space for an optimal policy or an optimal path to a goal. Actions cause transitions from state to state, and value functions are computed over states.</p></li>
<li><p><span class="math notranslate nohighlight">\(\star\textit{plan-space planning}\)</span>: search through the space of plans. Operators transform one plan into another, and value functions, if any, are defined over the space of plans. Plan-space methods are difficult to apply efficiently to the stochastic sequential decision problems that are the focus in reinforcement learning, and we do not consider them further</p></li>
</ul>
</li>
<li><p><strong>Unifying model-based and model-free RL methods</strong>: the heart of both learning and planning (state-space planning) methods is the <strong>1) estimation of value functions by 2) backing-up update operations</strong>. The difference is that whereas planning uses simulated experience generated by a model, learning methods use real experience generated by the environment.</p>
<ul class="simple">
<li><p>The common structure between planning and learning can be diagrammed as follows:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
		\text{model} \rightarrow \text{simulated experience} \xrightarrow{\text{backups}} \text{values} \rightarrow \text{policy}
	\]</div>
<ul>
<li><p>Example of planning: <span class="math notranslate nohighlight">\(\textit{random-sample one-step tabular}\)</span> <strong>Q-planning</strong></p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter7/algo_one_step_q_planning.png" alt="Algorithm: One-step Tabular Q-planning" style="width: 100%;">        
  </div>
</li>
</ul>
</li>
</ul>
</section>
<section id="dyna-integrated-planning-acting-and-learning">
<h2>7.2 Dyna: Integrated Planning, Acting, and Learning<a class="headerlink" href="#dyna-integrated-planning-acting-and-learning" title="Link to this heading">#</a></h2>
<ul>
<li><p><strong>An online planning agent</strong>: interacts with the environment in real time and updates the model based on the newly gained information.</p>
<ul>
<li><p>The possible relationships between experience, model, values, and policy are summarized in the following diagram:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter7/planning_agent.png" alt="Architecture of a planning agent" style="width: 39%;">        
  </div>
</li>
<li><p>Depending on how the real experience is used, the reinforcement learning process can be divided into direct and indirect RL:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\textit{direct reinforcement learning}\)</span>: uses real experience to directly improve the value function and policy.</p></li>
<li><p><span class="math notranslate nohighlight">\(\textit{indirect reinforcement learning}\)</span> (or <span class="math notranslate nohighlight">\(\textit{model-learning}\)</span>): uses real experience to improve the model.</p></li>
<li><p>Note: Indirect methods often make fuller use of a limited amount of experience and thus achieve a better policy with fewer environmental interactions. On the other hand, direct methods are much simpler and are not affected by biases in the design of the model. (Instead of exaggerating the contrast between these two, this book focuses more on discovering the similarities.)</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Dyna agent</strong>: a type of online planning agent that performs all the above processes — planning, acting, model-learning, and direct RL — all occurring continually.</p>
<ul>
<li><p>Architecture:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter7/dyna_agent_architecture.png" alt="Architecture of Dyna agent" style="width: 50%;">        
  </div>
</li>
<li><p>Explanation:</p>
<ul class="simple">
<li><p>The arrow on the left of the figure represents direct reinforcement learning operating on real experience to improve the value function and the policy.</p></li>
<li><p>On the right are model-based processes. The model is learned from real experience and gives rise to simulated experience.</p></li>
<li><p>Typically, the same reinforcement learning method is used both for learning from real experience and for planning from simulated experience.</p></li>
</ul>
</li>
<li><p>Computing time allocation:</p>
<ul class="simple">
<li><p>usually the acting, model-learning, and direct RL processes require little computation, and we assume they consume just a fraction of the time.</p></li>
<li><p>a larger fraction of time in each step can be devoted to the planning process, which is inherently computation-intensive.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>A type of dyna agent: Tabular Dyna-Q for estimating <span class="math notranslate nohighlight">\(q_\pi\)</span></strong>:</p>
<ul>
<li><p>Algorithm</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter7/algo_tabular_dyna_q.png" alt="Algorithm: Tabular Dyna-Q" style="width: 100%;">        
  </div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Model(s, a)\)</span> denotes the contents of the predicted next state and the reward received for state-action pair <span class="math notranslate nohighlight">\((s, a)\)</span>.</p></li>
<li><p>Direct reinforcement learning, model-learning are implemented by steps <code class="docutils literal notranslate"><span class="pre">(d)</span></code> and <code class="docutils literal notranslate"><span class="pre">(e)</span></code>, respectively.</p></li>
<li><p>The whole step <code class="docutils literal notranslate"><span class="pre">(f)</span></code> performs the planning.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\textit{Search Control}\)</span>: the first two steps in step <code class="docutils literal notranslate"><span class="pre">(f)</span></code> that select the starting states and actions for the simulated experiences generated by the model.</p></li>
<li><p>Note that the starting state in planning is random selected, it does NOT have to be the real starting state</p></li>
<li><p><strong>Note that planning only happens for the states with which the agent have made real experience.</strong></p></li>
</ul>
</li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">(e)</span></code> and <code class="docutils literal notranslate"><span class="pre">(f)</span></code> are omitted, the remaining algorithm would be one-step tabular Q-learning.</p></li>
</ul>
</div>
</li>
<li><p>Lecture video for an illustrative Example (starting from <code class="docutils literal notranslate"><span class="pre">3:13</span></code>): In a gridworld with obstacles, an agent starts from the lower left grid and travels to the end state on the upper right. Initial policy <span class="math notranslate nohighlight">\(\pi\)</span> assigns equal probability to all four action: left, right, up and down.</p>
  <a href="https://www.coursera.org/learn/sample-based-learning-methods/lecture/k7Out/the-dyna-algorithm">
  <img src="../_static/img/chapter7/dyna_q_example.png" alt="Video: The dyna algorithm" style="width:50%;">
  </a>
</li>
</ul>
</li>
<li><p><strong>Example: Dyna Maze</strong></p>
<ul>
<li><p>An illustrative example of how exactly a planning agent may learn much faster than a pure learning agent:</p>
  <a href="https://www.coursera.org/learn/sample-based-learning-methods/lecture/TGSQi/dyna-q-learning-in-a-simple-maze">
  <img src="../_static/img/chapter7/example_maze.png" alt="Video: The Maze example" style="width:60%;">
  </a>
</li>
<li><p>The agent needs many planning steps because search control samples state action pairs randomly. The planning update will not have any effect if the sample state action pair produces a 0 TD error. This happened a lot in this environment because all the rewards are 0, and so are the initial values.</p></li>
<li><p>In larger environments random search control becomes even more problematic. But in this example, the more planning we did the better the agent performed</p></li>
</ul>
</li>
</ul>
</section>
<section id="when-the-model-is-wrong">
<h2>7.3 When the model is wrong<a class="headerlink" href="#when-the-model-is-wrong" title="Link to this heading">#</a></h2>
<ul>
<li><p><strong>An inaccurate model</strong>: model is either incomplete (does not contain some transition information) or the environment has changed (leads the transition information stored in the model become inaccurate).</p>
<ul class="simple">
<li><p>When the model is wrong: we face the trade-off between exploration vs. exploitation (in the planning context):</p>
<ul>
<li><p>exploration means trying actions that improve the model</p></li>
<li><p>exploitation means behaving in the optimal way given the current model</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Example: Blocking Maze - when the environment become “worse”</strong></p>
<ul>
<li><p><strong>Description</strong>: initially, there is a short path from start to goal, to the right of the barrier, as shown in the upper left of the figure. After 1000 time steps, the short path is “blocked,” and a longer path is opened up along the left-hand side of the barrier</p>
  <img src="../_static/img/chapter7/example_blocking_maze.png" alt="The blocking maze example" style="width:60%;">
</li>
<li><p><strong>Interpretation</strong></p>
<ul class="simple">
<li><p>The first part of the graph shows that both Dyna agents found the short path within 1000 steps.</p></li>
<li><p>When the environment changed, the graphs become flat, indicating a period during which the agents obtained no reward because they were wandering around behind the barrier.</p></li>
<li><p>After a while, however, they were able to find the new opening and the new optimal behavior.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Example: Shortcut Maze - when the environment become “better”</strong></p>
<ul>
<li><p><strong>Description</strong>: initially, the optimal path is to go around the left side of the barrier (upper left). After 3000 steps, however, a shorter path is opened up along the right side, without disturbing the longer path</p>
  <img src="../_static/img/chapter7/example_shortcut_maze.png" alt="The shortcut maze example" style="width:60%;">
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is harder to detect modeling error in the case where the environment changes to become “better” (better solutions emerge), this is where more thoughts on exploration (during planning) are need to keep the model up-to-date.</p>
</div>
</li>
</ul>
</li>
<li><p><strong>Dyna-Q+ agent - encourages exploration during planning</strong></p>
<div class="admonition-todo admonition" id="id1">
<p class="admonition-title">Todo</p>
<p>Turn the dyna-Q+ algo to book-similar image</p>
</div>
<ul class="simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(Q(s,a)\)</span> and <span class="math notranslate nohighlight">\(Model(s,a)\)</span> for all <span class="math notranslate nohighlight">\(s \in S\)</span> and <span class="math notranslate nohighlight">\(a \in A\)</span></p></li>
<li><p>Loop forever:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">(a)</span></code> <span class="math notranslate nohighlight">\(S \leftarrow\)</span> current (non-terminal) state</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">(b)</span></code> <span class="math notranslate nohighlight">\(A \leftarrow \epsilon\)</span>-greedy(<span class="math notranslate nohighlight">\(S, Q\)</span>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">(c)</span></code> Take action <span class="math notranslate nohighlight">\(A\)</span>, observe reward <span class="math notranslate nohighlight">\(R\)</span>, and the next state <span class="math notranslate nohighlight">\(S'\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">(d)</span></code> <span class="math notranslate nohighlight">\(Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma max_a Q(S', a) - Q(S, A)]\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">(e)</span></code> <span class="math notranslate nohighlight">\(Model(S, A)\)</span> <span class="math notranslate nohighlight">\(\leftarrow R, S'\)</span>  and <span class="math notranslate nohighlight">\(S\)</span> <span class="math notranslate nohighlight">\(\leftarrow S'\)</span> (assuming deterministic environment)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">(f)</span></code> If <span class="math notranslate nohighlight">\(S\)</span> is the terminal state, then <span class="math notranslate nohighlight">\(S \leftarrow StartingState\)</span> and restart, Else loop repeat <span class="math notranslate nohighlight">\(n\)</span> times:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(S \leftarrow\)</span> random <strong>previously observed</strong> state</p></li>
<li><p><span class="math notranslate nohighlight">\(A \leftarrow\)</span> random action <strong>available (doen not have to be taken before)</strong> in <span class="math notranslate nohighlight">\(S\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(R, S' \leftarrow Model(S, A)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(R = R + \kappa \sqrt \tau_{S,A,R,S'}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma max_a Q(S', a) - Q(S, A)]\)</span></p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Modification 1: the reward is now <span class="math notranslate nohighlight">\(R + \kappa \sqrt \tau\)</span>, where <span class="math notranslate nohighlight">\(\kappa\)</span> is a constant and <span class="math notranslate nohighlight">\(\tau\)</span> indicates the amount of time steps that the transition <span class="math notranslate nohighlight">\((S,A) \rightarrow (R, S')\)</span> has not been tried during real experience.</p>
<ul>
<li><p>The more time that has elapsed, the greater (we might presume) the chance that the dynamics of this pair has changed and that the model of it is incorrect.</p></li>
<li><p>Note that <span class="math notranslate nohighlight">\(\tau\)</span> will not be updated in the planning process, but only in real interactions.</p></li>
<li><p>This type of way to encourage exploration is similar to Upper-Confidence-Bound Action Selection descibed in <a class="reference internal" href="2_multi_armed_bandits.html"><span class="std std-doc">Chapter 2</span></a> which both measure the uncertainty in a way.</p></li>
</ul>
</li>
<li><p>Modification 2: actions that had never been tried before from a state were allowed to be considered in the planning step <code class="docutils literal notranslate"><span class="pre">(f)</span></code>. The initial model for such actions was that they would lead back to the same state with a reward of zero.</p></li>
</ul>
</div>
</li>
</ul>
</section>
<section id="summary">
<h2>7.4 Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<ul>
<li><p><strong>Mindmap of where we are now</strong></p>
  <img src="../_static/img/chapter7/chapter7_mindmap.png" alt="Mindmap" style="width: 100%;"> 
</li>
<li><p><strong>Key Takeaways</strong></p>
<ol class="arabic simple">
<li><p>Model-Based vs. Model-Free Methods</p>
<ul class="simple">
<li><p>Model-based methods (e.g., Dynamic Programming) use planning with a model.</p></li>
<li><p>Model-free methods (e.g., Monte Carlo, TD) learn directly from experience.</p></li>
<li><p>Both compute value functions using backup operations but differ in the source of experience (simulated vs. real).</p></li>
</ul>
</li>
<li><p>Models and Planning</p>
<ul class="simple">
<li><p>A model predicts the environment’s response to actions.</p>
<ul>
<li><p>Distribution models provide all outcomes and probabilities.</p></li>
<li><p>Sample models provide a single sampled outcome.</p></li>
</ul>
</li>
<li><p>Planning improves policies using models.</p>
<ul>
<li><p>State-space planning searches state transitions for optimal policies.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Dyna: Integrated Planning, Acting, and Learning</p>
<ul class="simple">
<li><p>Dyna architecture combines direct reinforcement learning, model learning, and planning.</p>
<ul>
<li><p>Direct RL updates value functions from real experiences.</p></li>
<li><p>Model learning updates the model from real experiences.</p></li>
<li><p>Planning uses the model for simulated experiences to refine learning.</p></li>
</ul>
</li>
<li><p>Tabular Dyna-Q alternates between real and simulated experiences to accelerate learning.</p></li>
</ul>
</li>
<li><p>Handling Inaccurate Models</p>
<ul class="simple">
<li><p>Model inaccuracies occur when the environment changes or the model lacks information.</p></li>
<li><p>Planning faces a trade-off between exploration (improving the model) and exploitation (using the current model).</p></li>
</ul>
</li>
<li><p>Dyna-Q+ for Encouraging Exploration</p>
<ul class="simple">
<li><p>Dyna-Q+ adds exploration bonuses to rewards based on how long actions remain unvisited.</p></li>
<li><p>Untried actions are included in planning to promote broader exploration.</p></li>
<li><p>Helps agents adapt to dynamic environments by detecting changes faster.</p></li>
</ul>
</li>
<li><p>Practical Insights</p>
<ul class="simple">
<li><p>Incremental planning allows flexible adaptation to changes.</p></li>
<li><p>Model-based methods learn faster with fewer interactions, while model-free methods are simpler and avoid model bias.</p></li>
<li><p>Effective planning requires prioritizing useful simulated experiences through search control.</p></li>
</ul>
</li>
</ol>
</li>
<li><p><strong>Extra lecture video (optional)</strong>: <a class="reference external" href="https://www.coursera.org/learn/sample-based-learning-methods/lecture/pvjdV/drew-bagnell-self-driving-robotics-and-model-based-rl">Drew Bagnell: self-driving, robotics, and Model Based RL</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "Dong237/DistilRLIntroduction",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Contents"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
<div class="section ablog__blog_comments">
   
</div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="6_temporal_difference_learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 6. Temporal-Difference Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="8_on_policy_prediction_with_approximation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 8. On-policy Prediction with Approximation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#models-and-planning">7.1 Models and Planning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dyna-integrated-planning-acting-and-learning">7.2 Dyna: Integrated Planning, Acting, and Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-the-model-is-wrong">7.3 When the model is wrong</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">7.4 Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Youxiang Dong
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025, Youxiang Dong.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>