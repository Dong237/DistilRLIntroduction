
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 4. Dynamic Programming &#8212; DistilRLIntro 0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-examples.css?v=e236af4b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=e1a75a79"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Contents/4_dynamic_programming';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 5. Monte Carlo Methods" href="5_monte_carlo_methods.html" />
    <link rel="prev" title="Chapter 3. Finite Markov Decision Processes" href="3_markov_decision_process.html" /> 
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>   
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>
<aside class="bd-header-announcement" aria-label="Announcement">
  <div class="bd-header-announcement__content">⚠️This is an ongoing project and is currectly still under development (Chinese version on its way). ⚠️</div>
</aside>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="DistilRLIntro 0.1 documentation - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="DistilRLIntro 0.1 documentation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">I. Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="0_prelude.html">Chapter 0. Prelude</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_intro.html">Chapter 1. Introduction to RL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">II.1. Fundamentals of Reinforcement Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2_multi_armed_bandits.html">Chapter 2. Multi-armed Bandit</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_markov_decision_process.html">Chapter 3. Finite Markov Decision Processes</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 4. Dynamic Programming</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">II.2. Sample-based Learning Methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="5_monte_carlo_methods.html">Chapter 5. Monte Carlo Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="6_temporal_difference_learning.html">Chapter 6. Temporal-Difference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="7_planning_learning_acting.html">Chapter 7. Planning and Learning with Tabular Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">III.1. Value Function Approximation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="8_on_policy_prediction_with_approximation.html">Chapter 8. On-policy Prediction with Approximation</a></li>
<li class="toctree-l1"><a class="reference internal" href="9_on_policy_control_with_approximation.html">Chapter 9. On-policy Control with Approximation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">III.2. Policy Approximation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_policy_gradient_methods.html">Chapter 10. Policy Gradient Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_modern_policy_gradient_methods.html">Chapter 11. Modern Policy Gradient Methods</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/Dong237/DistilRLIntroduction" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Dong237/DistilRLIntroduction/blob/master/Contents/4_dynamic_programming.md?plain=1" target="_blank"
   class="btn btn-sm btn-source-file-button dropdown-item"
   title="Show source"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">Show source</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Dong237/DistilRLIntroduction/edit/master/Contents/4_dynamic_programming.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Dong237/DistilRLIntroduction/issues/new?title=Issue%20on%20page%20%2FContents/4_dynamic_programming.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Contents/4_dynamic_programming.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 4. Dynamic Programming</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-evaluation-prediction-problem">4.1 Policy Evaluation (Prediction problem)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-improvement">4.2 Policy Improvement</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-iteration-control">4.3 Policy Iteration (Control)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#value-iteration">4.4 Value Iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalized-policy-iteration-gpi">4.5 Generalized Policy Iteration (GPI)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">4.6 Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                   <section class="tex2jax_ignore mathjax_ignore" id="chapter-4-dynamic-programming">
<h1>Chapter 4. Dynamic Programming<a class="headerlink" href="#chapter-4-dynamic-programming" title="Link to this heading">#</a></h1>
<p>The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies <strong>given a perfect model of the environment</strong> as a Markov decision process (MDP), it therefore requires no interaction with the environment.</p>
<p>DP algorithms are limited in practice because of their assumption of a perfect model and because of their great computational expense. Yet DP provides an essential foundation for the understanding of the methods presented in the rest of this book. In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.</p>
<p>The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies. As we shall see, DP algorithms are obtained by turning Bellman equations such as these into assignments, that is, <strong>into update rules for improving approximations of the desired value functions.</strong></p>
<section id="policy-evaluation-prediction-problem">
<h2>4.1 Policy Evaluation (Prediction problem)<a class="headerlink" href="#policy-evaluation-prediction-problem" title="Link to this heading">#</a></h2>
<ul>
<li><p><strong>Policy Evaluation</strong>: to compute the state-value function <span class="math notranslate nohighlight">\(v_{\pi}\)</span> for an arbitrary policy <span class="math notranslate nohighlight">\(\pi\)</span>. (We also refer to this as the <span class="math notranslate nohighlight">\(\textit{prediction problem}\)</span>.)</p>
<ul>
<li><p><strong>DP assumes the environment’s dynamics are completely known</strong> (perfect model of the environment), in this case, we could use Bellman equation to form a simultaneous linear equation system with <span class="math notranslate nohighlight">\(|S|\)</span> equations and <span class="math notranslate nohighlight">\(|S|\)</span> unknowns.</p>
<p>With the initial approximation <span class="math notranslate nohighlight">\(v_0\)</span> chosen arbitrarily (except that the terminal state, if any, must be given value 0):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
        \begin{align*}
        v_{k+1}(s) &amp;= E_{\pi}[R_{t+1} + \gamma v_{k}(S_{t+1}) | S_t = s] \\
        &amp;= \sum_a \pi(a|s) \sum_{s', r}p(s', r|s, a) [r + \gamma v_{k}(s')] \text{ for all } s \in S
        \end{align*}
        \end{split}\]</div>
</li>
<li><p>The above algorithm is called <span class="math notranslate nohighlight">\(\textit{iterative policy evaluation}\)</span>. Letter <span class="math notranslate nohighlight">\(k\)</span> denotes the number of iterations.</p></li>
<li><p>All the updates done in DP algorithms are called <span class="math notranslate nohighlight">\(\textit{expected updates}\)</span> because they are based on an expectation over all possible next states rather than on a sample next state.</p></li>
<li><p>The updates of all states in one iteration is called a <span class="math notranslate nohighlight">\(\textit{sweep}\)</span> through the state space.</p></li>
</ul>
</li>
<li><p><strong>The Iterative Policy Evaluation algorithm for estimating <span class="math notranslate nohighlight">\(V \approx v_{\pi}\)</span></strong></p>
<ul>
<li><p>Algorithm:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter4/algo_iter_policy_eval.png" alt="Algorithm: Iterative Policy Evaluation" style="width: 100%;">
  </div>
</li>
<li><p>Intuition: <span class="math notranslate nohighlight">\(\Delta\)</span> will be set to 0 at the beginning of each iteration, and is used to record the <strong>maximal changes in all state values</strong> through state space. So when the maximal change in a sweep is smaller than the threshold <span class="math notranslate nohighlight">\(\theta\)</span>, the algorithm stops. We receive an approximate optimal value function <span class="math notranslate nohighlight">\(V \approx v_{\pi_\star}\)</span></p></li>
</ul>
</li>
<li><p><strong>Gridworld Example:</strong> watch this <a class="reference external" href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/ICAfp/iterative-policy-evaluation">lecture video</a> if you find the illustrations of sweeps during policy evaluation too hard to grasp.</p>
<ul>
<li><p><strong>Description</strong>:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter4/example4.1.1.png" alt="Gridworld" style="width: 70%;">
  </div>
<ul class="simple">
<li><p><strong>States</strong>: the nonterminal states are <span class="math notranslate nohighlight">\(S = {1, 2,...,14}\)</span>.</p></li>
<li><p><strong>Actions</strong>: there are four actions possible in each state, <span class="math notranslate nohighlight">\(A = \{up, down, right, left\}\)</span>, which deterministically causes the corresponding state transitions, except that actions that would take the agent off the grid in fact leave the state unchanged.</p>
<ul>
<li><p>For instance, <span class="math notranslate nohighlight">\(p(6, -1|5, right) = 1, p(7, -1|7, right) = 1\)</span>, and <span class="math notranslate nohighlight">\(p(10,r|5, right) = 0\)</span> for all <span class="math notranslate nohighlight">\(r \in R\)</span>.</p></li>
</ul>
</li>
<li><p><strong>Reward</strong>: this is an undiscounted, episodic task. The reward is <span class="math notranslate nohighlight">\(-1\)</span> on all transitions until the terminal state is reached. The expected reward function is thus <span class="math notranslate nohighlight">\(r(s, a, s')= -1\)</span> for all states <span class="math notranslate nohighlight">\(s, s'\)</span> and actions <span class="math notranslate nohighlight">\(a\)</span>.</p></li>
<li><p><strong>Policy</strong>: an equiprobable random policy (all actions are equally likely with probability of <span class="math notranslate nohighlight">\(0.25\)</span>)</p></li>
</ul>
</li>
<li><p><strong>Policy Evaluation</strong>:</p>
<ul>
<li><p>The first sweep:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter4/example4.1.2.png" alt="Step1.1" style="width: 75%;">        
  </div>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter4/example4.1.3.png" alt="Step1.2" style="width: 75%;">        
  </div>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter4/example4.1.4.png" alt="Step1.3" style="width: 75%;">        
  </div>
</li>
<li><p>The second sweep:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter4/example4.1.5.png" alt="Step2" style="width: 75%;">        
  </div>
</li>
<li><p>The third sweep:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter4/example4.1.6.png" alt="Step3" style="width: 75%;">        
  </div>
</li>
<li><p>…</p></li>
<li><p>The final sweep: <span class="math notranslate nohighlight">\(\Delta = 0\)</span> and is finally smaller than <span class="math notranslate nohighlight">\(\theta\)</span>, <span class="math notranslate nohighlight">\(V\)</span> and <span class="math notranslate nohighlight">\(V'\)</span> are both <span class="math notranslate nohighlight">\(V_\pi\)</span></p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter4/example4.1.7.png" alt="Final Step" style="width: 75%">        
  </div>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="policy-improvement">
<h2>4.2 Policy Improvement<a class="headerlink" href="#policy-improvement" title="Link to this heading">#</a></h2>
<ul>
<li><p><strong>Policy Improvement Theorem</strong>: If <span class="math notranslate nohighlight">\(q_{\pi}(s, \pi'(s)) \ge v_{\pi}(s)\)</span> holds for all <span class="math notranslate nohighlight">\(s \in S\)</span>, then the policy <span class="math notranslate nohighlight">\(\pi'\)</span> must be as good as, or better than, <span class="math notranslate nohighlight">\(\pi\)</span>. In other words, <span class="math notranslate nohighlight">\(v_{\pi'}(s) \ge v_{\pi}(s)\)</span> for all <span class="math notranslate nohighlight">\(s \in S\)</span> also holds.</p>
<ul>
<li><p>The intuition of the inequality <span class="math notranslate nohighlight">\(q_{\pi}(s, \pi'(s)) \ge v_{\pi}(s)\)</span> is that (recall the backup diagram starting with <span class="math notranslate nohighlight">\(v(s)\)</span> shown below) there exists one (or more) explicit action(s) that could bring more return for state <span class="math notranslate nohighlight">\(s\)</span> than simply calculating the expectation (since <span class="math notranslate nohighlight">\(v(s) = \sum_a \pi(a|s)q(s,a)\)</span>)</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter3/backup_diagram_v.png" alt="Backup diagram for v" style="width: 23%;">
  </div>
</li>
</ul>
</li>
<li><p><strong>Policy Improvement</strong>: the process of making a new policy that improves on an original policy, <strong>by making it greedy</strong> with respect to the value function of the original policy.</p>
<ul>
<li><p><strong>Greedification</strong>: with <span class="math notranslate nohighlight">\(\pi'\)</span> denoting the greedified policy:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
        \begin{align*}
        \pi'(s) &amp;= \arg\max_a q_{\pi}(s, a) \\
        &amp;= \arg\max_a E_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t=s] \\
        &amp;= \arg\max_a \sum_{s', r} p(s', r|s,a)[r + \gamma v_{\pi}(s')]
        \end{align*} 
        \end{split}\]</div>
</li>
<li><p><strong>Policy Improvement will lead to a strictly better policy</strong> unless the original policy is already optimal:</p>
<ul>
<li><p>suppose the new policy is as good as, but no better than the old policy (the policy can not be any better in the end), meaning <span class="math notranslate nohighlight">\(v_{\pi'}=v_{\pi}\)</span>, then:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
            \begin{align*}
            v_{\pi'} &amp;= \max_a \sum_{s', r} p(s', r|s,a)[r + \gamma v_{\pi}(s')] \\
            &amp;= \max_a \sum_{s', r} p(s', r|s,a)[r + \gamma v_{\pi'}(s')]
            \end{align*}
            \end{split}\]</div>
<p>The above last equation is exactly the Bellman optimality equation, this means that when the policy can not get any better, <span class="math notranslate nohighlight">\(v_{\pi'}\)</span> is <span class="math notranslate nohighlight">\(v_{\star}\)</span>, therefore, <span class="math notranslate nohighlight">\(\pi\)</span> and <span class="math notranslate nohighlight">\(\pi'\)</span> are both the optimal policy <span class="math notranslate nohighlight">\(\pi_{\star}\)</span>.</p>
</li>
<li><p><strong>If a policy is already the greedy policy with respect to its own value function, then this policy is the optimal policy</strong></p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Gridworld Example from <a class="reference internal" href="#policy-evaluation-prediction-problem">section 4.1</a> (continue, lecture video optional)</strong></p>
<ul>
<li><p>After getting the value function <span class="math notranslate nohighlight">\(v_{\pi}\)</span> of the initial random policy <span class="math notranslate nohighlight">\(\pi\)</span>, we perform greedification to get the new policy <span class="math notranslate nohighlight">\(\pi'\)</span> (illustrated by white arrows), which is strictly better than <span class="math notranslate nohighlight">\(\pi\)</span> according to the nature of policy improvement.</p>
  <a href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/FVd6r/policy-improvement">
  <img src="../_static/img/chapter4/example4.2.png" alt="Policy Improvement" style="width: 75%"> 
  </a>
</li>
<li><p>In doing so, we now have successfully improved our original policy (for one iteration).</p></li>
</ul>
</li>
</ul>
</section>
<section id="policy-iteration-control">
<h2>4.3 Policy Iteration (Control)<a class="headerlink" href="#policy-iteration-control" title="Link to this heading">#</a></h2>
<ul>
<li><p><strong>Policy iteration</strong>: the process of performing policy evaluation and policy improvement iteratively to find the optimal policy.</p>
<ul class="simple">
<li><p>Because a finite MDP has only a finite number of policies, this process must converge to an optimal policy and optimal value function in a finite number of iterations. With <span class="math notranslate nohighlight">\(E\)</span> stands for evaluation and <span class="math notranslate nohighlight">\(I\)</span> for improvement, the process is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \pi_0 \xrightarrow{\text{E}} v_{\pi_0} \xrightarrow{\text{I}} \pi_1 \xrightarrow{\text{E}} v_{\pi_1} \xrightarrow{\text{I}} ... \xrightarrow{\text{I}} \pi_{\star} \xrightarrow{\text{E}} v_{\star}
    \]</div>
</li>
<li><p><strong>Algorithm for Policy Iteration</strong>:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter4/algo_policy_iteration.png" alt="Algorithm: Policy Iteration" style="width: 100%;">
  </div>
</li>
<li><p><strong>New Gridworld Example</strong>: Again, if you find this all too abstract, watch this <a class="reference external" href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/Xv32P/policy-iteration">lecture video</a> which should give you a more slow and smooth illustration.</p>
<ul>
<li><p><strong>Description</strong>:</p>
<ul>
<li><p>The Gridworld example in <a class="reference internal" href="#policy-evaluation-prediction-problem">section 4.1</a> actually reaches the optimal policy after only one iteration, we now make the example a bit more complex by eliminating one terminal grid and adding blue states where rewards have much lower value of <span class="math notranslate nohighlight">\(-10\)</span>.</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter4/example4.3.1.png" alt="New Gridworld" style="width: 380px;">        
  </div>
</li>
</ul>
</li>
<li><p><strong>Policy iteration</strong>:</p>
<ul>
<li><p>The first iteration:</p>
  <div style="display: flex; justify-content: center; gap: 20px;">
  <div style="display: flex; flex-direction: column; align-items: center;">
  <h4>Evaluation</h4>
  <img src="../_static/img/chapter4/example4.3.2.png" alt="Evaluation" style="width: 200px;">
  </div>
  <div style="display: flex; flex-direction: column; align-items: center;">
  <h4>Improvement</h4>
  <img src="../_static/img/chapter4/example4.3.3.png" alt="Improvement" style="width: 200px;">
  </div>
  </div>
</li>
<li><p>The second iteration:</p>
<ul>
<li><p>Pay attention to how exactly the greedification is performed (e.g., the second left grid on the last row).</p>
<p>Remember that <span class="math notranslate nohighlight">\(\pi'(s) = \arg\max_a \sum_{s', r} p(s', r|s,a)[r + \gamma v_{\pi}(s')]\)</span> not <span class="math notranslate nohighlight">\(\pi'(s) = \arg\max_a v(s')\)</span>, so don’t just greedify an action towards a state <span class="math notranslate nohighlight">\(s'\)</span> simply because that state has a higher value <span class="math notranslate nohighlight">\(v(s')\)</span>.</p>
</li>
</ul>
  <div style="display: flex; justify-content: center; gap: 20px;">
  <div style="display: flex; flex-direction: column; align-items: center;">
  <h4>Evaluation</h4>
  <img src="../_static/img/chapter4/example4.3.4.png" alt="Evaluation" style="width: 200px;">
  </div>
  <div style="display: flex; flex-direction: column; align-items: center;">
  <h4>Improvement</h4>
  <img src="../_static/img/chapter4/example4.3.5.png" alt="Improvement" style="width: 200px;">
  </div>
  </div>
</li>
<li><p>…</p></li>
<li><p>The final iteration</p>
<ul class="simple">
<li><p>Note that in the final iteration, after policy improvement, the policy remains the same, i.e., the original policy (before improvement) is already the greedy policy with respect to its own value function, the optimal policy is found.</p></li>
</ul>
  <div style="display: flex; justify-content: center; gap: 20px;">
  <div style="display: flex; flex-direction: column; align-items: center;">
  <h4>Evaluation</h4>
  <img src="../_static/img/chapter4/example4.3.6.png" alt="Evaluation" style="width: 200px;">
  </div>
  <div style="display: flex; flex-direction: column; align-items: center;">
  <h4>Improvement</h4>
  <img src="../_static/img/chapter4/example4.3.7.png" alt="Improvement" style="width: 200px;">
  </div>
  </div>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="value-iteration">
<h2>4.4 Value Iteration<a class="headerlink" href="#value-iteration" title="Link to this heading">#</a></h2>
<ul>
<li><p><strong>Value Iteration</strong>:  is the special case of Policy Iteration where policy evaluation stops after just one sweep.</p>
<ul class="simple">
<li><p>Drawback of policy iteration: requires iterative computation, each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set, and convergence only occurs in the limit, which takes a lot of time.</p></li>
<li><p>Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement.</p></li>
</ul>
</li>
<li><p><strong>Update rule</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    V_{k+1}(s) \ &amp;\dot= \ \max_a q_{k}(s, a) \\
    &amp;= \max_a \sum_{s', r} p(s', r|s,a) [r + \gamma V_{k}(s')]\ \text{for all} s \in S
    \end{align*}
    \end{split}\]</div>
<ul class="simple">
<li><p>Note that the above equation is obtained simply by turning the Bellman Optimality Equation into an updating rule.</p></li>
</ul>
</li>
<li><p><strong>Algorithm for Value Iteration</strong>:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter4/algo_value_iteration.png" alt="Algorithm: Value Iteration" style="width: 100%;">
  </div>
</li>
</ul>
</section>
<section id="generalized-policy-iteration-gpi">
<h2>4.5 Generalized Policy Iteration (GPI)<a class="headerlink" href="#generalized-policy-iteration-gpi" title="Link to this heading">#</a></h2>
<ul>
<li><p><span class="math notranslate nohighlight">\(\star\)</span> <strong>Two types of DP</strong>:</p>
<ul class="simple">
<li><p>Synchronous DP: update all states systematically in a certain order (takes a very long for large state space)</p></li>
<li><p>Asynchronous DP: update states without order (can be faster, but also problematic when only a small set of states is being updated constantly)</p></li>
</ul>
</li>
<li><p><strong>Generalized policy iteration (GPI)</strong>: refers to the general idea of letting policy-evaluation and policy improvement processes interact, independent of the granularity and other details of the two processes.</p>
<ul>
<li><p>Almost all reinforcement learning methods are well described as GPI. That is, all have identifiable policies and value functions, with the policy always being improved with respect to the value function and the value function always being driven toward the value function for the policy:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter4/gpi2.png" alt="Generalized policy iteration" style="width: 50%;;">
  </div>
<p>Both processes stabilize only when a policy has been found that is greedy with respect to its own evaluation function (when <span class="math notranslate nohighlight">\(\pi\)</span> itself is the greedy policy of <span class="math notranslate nohighlight">\(v_{\pi}\)</span>). This implies that the Bellman optimality equation for state-value functions holds, and thus that the policy and the value function are optimal.</p>
</li>
</ul>
</li>
</ul>
</section>
<section id="summary">
<h2>4.6 Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>Classical DP methods operate in sweeps through the state set, performing an <span class="math notranslate nohighlight">\(\textit{expected update}\)</span> operation on each state. The update of states is based on estimates of the values of successor states. That is, estimates are updated on the basis of other estimates. We call this general idea <span class="math notranslate nohighlight">\(\textit{bootstrapping}\)</span> (a very fundamental concept in many RL algorithms, and we will introduce it in depth in <a class="reference internal" href="6_temporal_difference_learning.html"><span class="std std-doc">Chapter 6</span></a>) and requires a perfect model of the environment.</p>
<p>In the next chapter we explore Monte Carlo method - a reinforcement learning method that does not require a model and do not bootstrap. But for now, a quick summary:</p>
<ul>
<li><p><strong>Mindmap of where we are now</strong>:</p>
  <img src="../_static/img/chapter4/chapter4_mindmap.png" alt="Mindmap" style="width:100%;">
</li>
<li><p><strong>Key Takeaways</strong>:</p>
<ol class="arabic">
<li><p><strong>DP Overview:</strong></p>
<ul class="simple">
<li><p>Computes optimal policies using a perfect model of the environment (MDP).</p></li>
<li><p>No interaction with the environment; computationally expensive but foundational for RL.</p></li>
<li><p>Core idea is to use value functions and Bellman equations as update rules to improve policies.</p></li>
</ul>
</li>
<li><p><strong>Policy Evaluation (Prediction):</strong></p>
<ul>
<li><p>Iteratively estimate <span class="math notranslate nohighlight">\(v_\pi\)</span> for a given policy <span class="math notranslate nohighlight">\(\pi\)</span>:</p>
<div class="math notranslate nohighlight">
\[
            v_{k+1}(s) = \sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_k(s')]
            \]</div>
</li>
<li><p>Stop when changes in <span class="math notranslate nohighlight">\(v(s)\)</span> are smaller than a threshold <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ul>
</li>
<li><p><strong>Policy Improvement:</strong></p>
<ul>
<li><p>Create a greedy policy <span class="math notranslate nohighlight">\(\pi'\)</span> with respect to <span class="math notranslate nohighlight">\(v_\pi\)</span>:</p>
<div class="math notranslate nohighlight">
\[
            \pi'(s) = \arg\max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')]
            \]</div>
</li>
<li><p>If no further improvement is possible, <span class="math notranslate nohighlight">\(\pi\)</span> is optimal.</p></li>
</ul>
</li>
<li><p><strong>Policy Iteration:</strong></p>
<ul>
<li><p>Alternate between policy evaluation and improvement until convergence:</p>
<div class="math notranslate nohighlight">
\[
            \pi_0 \rightarrow v_{\pi_0} \rightarrow \pi_1 \rightarrow v_{\pi_1} \rightarrow \ldots \rightarrow \pi_* \rightarrow v_*
            \]</div>
</li>
</ul>
</li>
<li><p><strong>Value Iteration:</strong></p>
<ul>
<li><p>Simplified policy iteration with one sweep per iteration:</p>
<div class="math notranslate nohighlight">
\[
            V_{k+1}(s) = \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma V_k(s')]
            \]</div>
</li>
<li><p>Converges to optimal <span class="math notranslate nohighlight">\(V\)</span> and greedy policy <span class="math notranslate nohighlight">\(\pi_*\)</span>.</p></li>
</ul>
</li>
<li><p><strong>Generalized Policy Iteration (GPI):</strong></p>
<ul class="simple">
<li><p>Continuous interaction of policy evaluation and improvement.</p></li>
<li><p>Stabilizes when the policy is greedy with respect to its own value function.</p></li>
</ul>
</li>
</ol>
</li>
<li><p><strong>Extra lecture video (optional)</strong>: <a class="reference external" href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/phTdz/warren-powell-approximate-dynamic-programming-for-fleet-management-long">Warren Powell: Approximate Dynamic Programming for Fleet Management (Long)</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "Dong237/DistilRLIntroduction",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Contents"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
<div class="section ablog__blog_comments">
   
</div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="3_markov_decision_process.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 3. Finite Markov Decision Processes</p>
      </div>
    </a>
    <a class="right-next"
       href="5_monte_carlo_methods.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 5. Monte Carlo Methods</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-evaluation-prediction-problem">4.1 Policy Evaluation (Prediction problem)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-improvement">4.2 Policy Improvement</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-iteration-control">4.3 Policy Iteration (Control)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#value-iteration">4.4 Value Iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalized-policy-iteration-gpi">4.5 Generalized Policy Iteration (GPI)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">4.6 Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Youxiang Dong
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025, Youxiang Dong.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>