# Chapter 8. Planning and Learning with Tabular Methods

In this chapter we develop a unified view of reinforcement learning methods that require a model of the environment ($\textit{model-based}$ RL methods), such as dynamic programming and heuristic search, and methods that can be used without a model ($\textit{model-free}$ RL methods), such as Monte Carlo and temporal-difference methods. 

**Model-based methods rely on $\textit{planning}$** as their primary component, while **model-free methods primarily rely on $\textit{learning}$**. Although there are real differences between these two kinds of methods, there are also great similarities:
- The heart of both kinds of methods is the computation of value functions. 
- All the methods are based on looking ahead to future events, computing a backed-up value, and then using it as an update target for an approximate value function.

In addition to the unified view of planning and learning methods, a second theme in this chapter is the benefits of planning in small, incremental steps. This enables planning to be interrupted or redirected at any time with little wasted computation, which appears to be a key requirement for effciently intermixing planning with acting and with learning of the model.

## 8.1 Models and Planning

- $\textit{Model}$ (of the environment): anything that an agent can use to predict how the environment will respond to its actions, 

	- Distribution models: produce a descriptioin of all possibilities and their probabilities. E.g., the model used in DP (i.e., the dynamics of the environment: $p(r, s\prime | s, a)$)

	- Sample models: produce just one of the possibilities, sampled according to the probabilities. E.g., the kind of model used in the blackjack example in Chapter 5 (<span style="color:red;">A link here</span>) is a sample model.

	- Both kinds of models are used to $\textit{simulate}$ the environment and produce $\textit{simulated experience}$. Given a starting state and a policy, a sample model could produce an entire episode, and a distribution model could generate all possible episodes and their probabilities. 


- $\textit{Planning}$: any computational process that takes a $\textit{model}$ as input and produces or improves a policy for interacting with the modeled environment, illustrated as:
	$$
		model \xrightarrow{\text{planning}} policy
	$$

	there are two distinct approaches to planning:

	- $\textit{state-space planning}$: a search through the state space for an optimal policy or an optimal path to a goal. Actions cause transitions from state to state, and value functions are computed over states.
	- $\textit{plan-space planning*}$: search through the space of plans. Operators transform one plan into another, and value functions, if any, are defined over the space of plans. Plan-space methods are diffcult to apply effciently to the stochastic sequential decision problems that are the focus in reinforcement learning, and we do not consider them further


- Unifying model-based and model-free RL methods: the heart of both learning and planning (state-space planning) methods is the **1) estimation of value functions by 2) backing-up update operations**. The difference is that whereas planning uses simulated experience generated by a model, learning methods use real experience generated by the environment.

	- The common structure between planning and learning can be diagrammed as follows:

	$$
		\text{model} \rightarrow \text{simulated experience} \xrightarrow{\text{backups}} \text{values} \rightarrow \text{policy}
	$$

	- Example of planning: $\textit{random-sample one-step tabular}$ **Q-planning**
		- Loop forever:
			- 1. Select a state, S and A at random
			- 2. Send S, A to a sample model, and obtain a sample next reward R, and a sample next state, $S\prime$
			- 3. Apply **one-step tabular Q-learning** to S, A ,R, $S\prime$: 
				$Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma max_a Q(S\prime, a) - Q(S, A)]$
			
## 8.2 Dyna: Integrated Planning, Acting, and Learning


- direct vs. indirect reinforcement learning:
	- direct RL: use real experience to directly improve the valua function and policy
	- indirect RL: use real experience to improve the model (involved in planning)

- Dyna-Q: 
	- model $\rightarrow$ value/policy $\rightarrow$ (real) experience $\rightarrow$ model $\rightarrow$ ... (see the circle graph in section 8.2)
	- On each arrow above: planning $\rightarrow$ acting $\rightarrow$ model learning  $\rightarrow$ ...
	
- Dyna agent: see book figure 8.1
	- usually the acting, model-learning, and direct RL processes require little computation, and we assum they consume just a fraction of the time. The remaining time in each step can be devoted to the planning process, which is inherently computation-intensive.

- Tabular Dyna-Q:

	- Initialize Q(s,a) and Model(s,a) for all $s \in S$ and $a \in A$ 
	- Loop forever:
		- `(a)` $S \leftarrow$ current (nonterminal) state
		- `(b)` $A \leftarrow \epsilon$-greedy(S, Q)
		- `(c)` Take action A, observe reward R, and the next state $S\prime$
		- `(d)` $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma max_a Q(S\prime, a) - Q(S, A)]$
		- `(e)` Model(S, A) $\leftarrow R, S\prime$ (assuming deterministic environment)
		- `(f)` Loop repeat n times:
			- S $S \leftarrow$ random **previously observed** state
			- A $S \leftarrow$ random action **previously taken** in S
			- $R, S\prime \leftarrow Model(S, A)$
			- $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma max_a Q(S\prime, a) - Q(S, A)]$

- Note:
	- Model(s, a) denotes the contents of the predicted next state and the reward reveived for state-action pair (s, a)
	- Step (d) indicates direct Reinforcement Learning, step (e) is the model-learning, step (f) is the planning process.
	- If (e) and (f) is ommited, the remaining algo would be one-step tabular Q-learning
	- Note that Dyna-Q performs n updates for each environment transition
	
	
## When the model is wrong

- Inaccurate model: model is incomplete or the environment has changed
	- It is harder to detect modeling error in the case where the environment changes to become "better" (better solutions emerge) than the environment becomes "worse"
	- see book figure 8.5, the dyna-Q agent would only discover the existence of the better solution with a very low probability (but still, it is possible) since the model does have that new information

- exploration vs. exploitation (in the planning context):
	- exploration means trying actions that improve the model
	- exploitation means behaving in the optimal way given the current model

- Dyna-Q+ agent
	- keeps track for each state-action pair of how many time steps have elapsed since the pair was last tried in a real interaction with the environment
	- modifies the reward to $r + \kappa \sqrt \tau$, where $\kappa$ is the constant, $\tau$ indicates the amount of time steps that the transition has not been tried. 
	- note that $\tau$ will not be updated in the planning process, but only in real interactions.
