## Models and Planning

- model-based vs. model-free RL:
	- model-based methods rely on planning
		- *Planning* refers to any computational process that takes a model as input and produces or improves a policy for interacting with the modeled environment
		- only *state-space planning* is considered in this book. In state-space planning, actions cause transitions from state to state, and value functions are computed over states.
	- model-free methods rely on learning

- The heart of both learning and planning methods is the estimation of value functions by backing-up update operations. The difference is that whereas planning uses simulated experience generated by a model, learning methods use real experience generated by the environment.


- model of the environment: anything that an agent can use to predict how the environment will respond to its actions (used to **simulate** the environment).


- distribution models vs. sample models:
	- distribution models: produce a descriptioin of all possibilities and their probabilities. E.g., the model used in DP (i.e., the dynamics of the environment: $p(r, s\prime | s, a)$)
	- sample models: produce just one of the possibilities, sampled according to the probabilities.

- Example: random-sample one-step tabular **Q-planning**
	- Loop forever:
		- 1. Select a state, S and A at random
		- 2. Send S, A to a sample model, and obtain a sample next reward R, and a sample next state, $S\prime$
		- 3. Apply **one-step tabular Q-learning** to S, A ,R, $S\prime$: 
			$Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma max_a Q(S\prime, a) - Q(S, A)]$
			
## Dyna: Integrated Planning, Acting, and Learning


- direct vs. indirect reinforcement learning:
	- direct RL: use real experience to directly improve the valua function and policy
	- indirect RL: use real experience to improve the model (involved in planning)

- Dyna-Q: 
	- model $\rightarrow$ value/policy $\rightarrow$ (real) experience $\rightarrow$ model $\rightarrow$ ... (see the circle graph in section 8.2)
	- On each arrow above: planning $\rightarrow$ acting $\rightarrow$ model learning  $\rightarrow$ ...
	
- Dyna agent: see book figure 8.1
	- usually the acting, model-learning, and direct RL processes require little computation, and we assum they consume just a fraction of the time. The remaining time in each step can be devoted to the planning process, which is inherently computation-intensive.

- Tabular Dyna-Q:

	- Initialize Q(s,a) and Model(s,a) for all $s \in S$ and $a \in A$ 
	- Loop forever:
		- `(a)` $S \leftarrow$ current (nonterminal) state
		- `(b)` $A \leftarrow \epsilon$-greedy(S, Q)
		- `(c)` Take action A, observe reward R, and the next state $S\prime$
		- `(d)` $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma max_a Q(S\prime, a) - Q(S, A)]$
		- `(e)` Model(S, A) $\leftarrow R, S\prime$ (assuming deterministic environment)
		- `(f)` Loop repeat n times:
			- S $S \leftarrow$ random **previously observed** state
			- A $S \leftarrow$ random action **previously taken** in S
			- $R, S\prime \leftarrow Model(S, A)$
			- $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma max_a Q(S\prime, a) - Q(S, A)]$

- Note:
	- Model(s, a) denotes the contents of the predicted next state and the reward reveived for state-action pair (s, a)
	- Step (d) indicates direct Reinforcement Learning, step (e) is the model-learning, step (f) is the planning process.
	- If (e) and (f) is ommited, the remaining algo would be one-step tabular Q-learning
	- Note that Dyna-Q performs n updates for each environment transition
	
	
## When the model is wrong

- Inaccurate model: model is incomplete or the environment has changed
	- It is harder to detect modeling error in the case where the environment changes to become "better" (better solutions emerge) than the environment becomes "worse"
	- see book figure 8.5, the dyna-Q agent would only discover the existence of the better solution with a very low probability (but still, it is possible) since the model does have that new information

- exploration vs. exploitation (in the planning context):
	- exploration means trying actions that improve the model
	- exploitation means behaving in the optimal way given the current model

- Dyna-Q+ agent
	- keeps track for each state-action pair of how many time steps have elapsed since the pair was last tried in a real interaction with the environment
	- modifies the reward to $r + \kappa \sqrt \tau$, where $\kappa$ is the constant, $\tau$ indicates the amount of time steps that the transition has not been tried. 
	- note that $\tau$ will not be updated in the planning process, but only in real interactions.
