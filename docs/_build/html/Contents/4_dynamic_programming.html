
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 4. Dynamic Programming &#8212; DistilRLIntro 0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=e1a75a79"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Contents/4_dynamic_programming';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 5. Monte Carlo Methods" href="5_monte_carlo_methods.html" />
    <link rel="prev" title="Chapter 3. Finite Markov Decision Processes" href="3_markov_decision_process.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">DistilRLIntro 0.1 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="0_prelude.html">Why this project</a></li>



<li class="toctree-l1"><a class="reference internal" href="1_intro.html">Chapter 1. Introduction to RL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Fundamentals of Reinforcement Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2_multi_armed_bandits.html">Chapte 2. Multi-armed Bandit</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_markov_decision_process.html">Chapter 3. Finite Markov Decision Processes</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 4. Dynamic Programming</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Sample-based Learning Methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="5_monte_carlo_methods.html">Chapter 5. Monte Carlo Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="6_temporal_difference_learning.html">Chapter 6. Temporal-Difference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="7_planning_learning_acting.html">Chapter 7. Planning and Learning with Tabular Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Value Function Approximation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="8_on_policy_prediction_with_approximation.html">Chapter 8. On-policy Prediction with Approximation</a></li>
<li class="toctree-l1"><a class="reference internal" href="9_on_policy_control_with_approximation.html">Chapter 9. On-policy Control with Approximation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Policy Approximation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_policy_gradient_methods.html">Chapter 10. Policy Gradient Methods</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Contents/4_dynamic_programming.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 4. Dynamic Programming</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-evaluation-prediction-problem">4.1 Policy Evaluation (Prediction problem)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-improvement">4.2 Policy Improvement</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-iteration-control">4.3 Policy Iteration (Control)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#value-iteration">4.4 Value Iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalized-policy-iteration-gpi">4.5 Generalized Policy Iteration (GPI)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">4.6 Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="chapter-4-dynamic-programming">
<h1>Chapter 4. Dynamic Programming<a class="headerlink" href="#chapter-4-dynamic-programming" title="Link to this heading">#</a></h1>
<p>The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies <strong>given a perfect model of the environment</strong> as a Markov decision process (MDP), it therefore requires no interaction with the environment.</p>
<p>DP algorithms is limited in practice because of their assumption of a perfect model and because of their great computational expense. Yet DP provides an essential foundation for the understanding of the methods presented in the rest of this book. In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.</p>
<p>The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies. As we shall see, DP algorithms are obtained by turning Bellman equations such as these into assignments, that is, <strong>into update rules for improving approximations of the desired value functions.</strong></p>
<section id="policy-evaluation-prediction-problem">
<h2>4.1 Policy Evaluation (Prediction problem)<a class="headerlink" href="#policy-evaluation-prediction-problem" title="Link to this heading">#</a></h2>
<ul>
<li><p>$\textit{Policy Evaluation}$: to compute the state-value function $v_{\pi}$ for an arbitrary policy $\pi$. (We also refer to this as the $\textit{prediction problem}$.)</p>
<ul>
<li><p>DP assumes the environment’s dynamics are completely known (perfect model of the environment), in this case, we could use bellman equation to form a simultaneous linear equation system with $|S|$ equations and $|S|$ unknowns.</p>
<p>With the initial approximation $v_0$ chosen arbitrarily (except that the terminal state, if any, must be given value 0):</p>
<p>$$
\begin{align*}
v_{k+1}(s) &amp;= E_{\pi}[R_{t+1} + \gamma v_{k}(S_{t+1}) | S_t = s] \
&amp;= \sum_a \pi(a|s) \sum_{s’, r}p(s’, r|s, a) [r + \gamma v_{k}(s’)] \text{ for all } s \in S
\end{align*}
$$</p>
</li>
<li><p>The above algorithm is called $\textit{iterative policy evaluation}$. Letter $k$ denotes the number of iterations.</p></li>
<li><p>All the updates done in DP algorithms are called $\textit{expected updates}$ because they are based on an expectation over all possible next states rather than on a sample next state.</p></li>
<li><p>The updates of all states in one iteration is called a $\textit{sweep}$ through the state space.</p></li>
</ul>
</li>
<li><p>The Iterative Policy Evaluation algorithm for estimating $V \approx v_{\pi}$</p>
<ul class="simple">
<li><p>Algo:</p>
<ul>
<li><p>Initialization:</p>
<ul>
<li><p>Input: $\pi$, the policy to be evaluated</p></li>
<li><p>Algorithm Parameters: A small threshold $\theta &gt; 0$ determining the accuracy of estimation</p></li>
<li><p>Initialize $V(s)$ for all $s \in S$, arbitrarily, except that $V(terminal) = 0$</p></li>
</ul>
</li>
<li><p>Loop: (for each iteration)</p>
<ul>
<li><p>$\Delta = 0$</p></li>
<li><p>Loop for each $s \in S$:</p>
<ul>
<li><p>$v \leftarrow V(s)$</p></li>
<li><p>$V(s) = \sum_a \pi(a|s) \sum_{s’, r} p(s’,r|s,a) [r + \gamma V(s’)]$</p></li>
<li><p>$\Delta \leftarrow \max(\Delta, |v - V(s)|)$</p></li>
</ul>
</li>
<li><p>Until $\Delta &lt; \theta$</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Intuition: $\Delta$ will be set to 0 at the beginning of each iteration, and is used to record the <strong>maximal changes in all state values</strong> through state space. So when the maximal change in a sweep is smaller than the threshold $\theta$, the algorithm stops. We receive a approximate optimal value function $V \approx v_{\pi_\star}$</p></li>
</ul>
</li>
<li><p>Gridworld Example: watch this <a class="reference external" href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/ICAfp/iterative-policy-evaluation">lecture video</a> if you find the illustrations of sweeps during policy evaluation too hard to grasp.</p>
<ul>
<li><p>Description:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../img/chapter4/example4.1.1.png" alt="Gridworld" style="width: 70%;">
  </div>
<ul class="simple">
<li><p>States: the nonterminal states are $S = {1, 2,…,14}$.</p></li>
<li><p>Actions: there are four actions possible in each state, $A = {up, down, right, left}$, which deterministically cause the corresponding state transitions, except that actions that would take the agent off the grid in fact leave the state unchanged.</p>
<ul>
<li><p>For instance, $p(6, -1|5, right) = 1, p(7, -1|7, right) = 1$, and $p(10,r|5, right) = 0$ for all $r \in R$.</p></li>
</ul>
</li>
<li><p>Reward: this is an undiscounted, episodic task. The reward is $-1$ on all transitions until the terminal state is reached. The expected reward function is thus $r(s, a, s’)= -1$ for all states $s, s’$ and actions $a$.</p></li>
<li><p>Policy: an equiprobable random policy (all actions are equally likely with probability of $0.25$)</p></li>
</ul>
</li>
<li><p>Policy Evaluation:</p>
<ul>
<li><p>The first sweep:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../img/chapter4/example4.1.2.png" alt="Step1.1" style="width: 75%;">        
  </div>
  <div style="display: flex; justify-content: center;">
  <img src="../img/chapter4/example4.1.3.png" alt="Step1.2" style="width: 75%;">        
  </div>
  <div style="display: flex; justify-content: center;">
  <img src="../img/chapter4/example4.1.4.png" alt="Step1.3" style="width: 75%;">        
  </div>
</li>
<li><p>The second sweep:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../img/chapter4/example4.1.5.png" alt="Step2" style="width: 75%;">        
  </div>
</li>
<li><p>The third sweep:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../img/chapter4/example4.1.6.png" alt="Step3" style="width: 75%;">        
  </div>
</li>
<li><p>…</p></li>
<li><p>The final sweep: $\Delta = 0$ and is finally smaller than $\theta$, $V$ and $V’$ are both $V_\pi$</p>
  <div style="display: flex; justify-content: center;">
  <img src="../img/chapter4/example4.1.7.png" alt="Final Step" style="width: 75%">        
  </div>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="policy-improvement">
<h2>4.2 Policy Improvement<a class="headerlink" href="#policy-improvement" title="Link to this heading">#</a></h2>
<ul>
<li><p>$\textit{Policy Improvement Theorem}$: if $q_{\pi}(s, \pi’(s)) \ge v_{\pi}(s)$ holds for all $s \in S$, then the policy $\pi’$ must be as good as, or better than, $\pi$. In other words, $v_{\pi’}(s) \ge v_{\pi}(s)$ for all $s \in S$ also holds.</p>
<ul>
<li><p>the intuition of the inequality $q_{\pi}(s, \pi’(s)) \ge v_{\pi}(s)$ is that (recall the backup diagram starting with $v(s)$ shown below) there exists one (or more) explicit action(s) that could bring more return for state $s$ than simply calculating the expectation (since $v(s) = \sum_a \pi(a|s)q(s,a)$)</p>
  <div style="display: flex; justify-content: center;">
  <img src="../img/chapter3/backup_diagram_v.png" alt="Backup diagram for v" style="width: 23%;">
  </div>
</li>
</ul>
</li>
<li><p>$\textit{Policy Improvement}$: the process of making a new policy that improves on an original policy, <strong>by making it greedy</strong> with respect to the value function of the original policy.</p>
<ul>
<li><p>Greedification: with $\pi’$ denoting the greedified policy:
$$
\begin{align*}
\pi’(s) &amp;= \arg\max_a q_{\pi}(s, a) \
&amp;= \arg\max_a E_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t=s] \
&amp;= \arg\max_a \sum_{s’, r} p(s’, r|s,a)[r + \gamma v_{\pi}(s’)]
\end{align*}
$$</p></li>
<li><p><strong>Policy Improvement will lead to a strictly better policy</strong> unless the original policy is already optimal:</p>
<ul>
<li><p>suppose the new policy is as good as, but no better than the old policy (the policy can not be any better in the end), meaning $v_{\pi’}=v_{\pi}$, then:</p>
<p>$$
\begin{align*}
v_{\pi’} &amp;= \max_a \sum_{s’, r} p(s’, r|s,a)[r + \gamma v_{\pi}(s’)] \
&amp;= \max_a \sum_{s’, r} p(s’, r|s,a)[r + \gamma v_{\pi’}(s’)]
\end{align*}
$$</p>
<p>The above last equation is exactly the bellman optimality equation, this means that when the policy can not get any better, $v_{\pi’}$ is $v_{\star}$, therefore, $\pi$ and $\pi’$ are both the optimal policy $\pi_{\star}$.</p>
</li>
<li><p><strong>if a policy is already the greedy policy with respect to its own value function, then this policy is the optimal policy</strong></p></li>
</ul>
</li>
</ul>
</li>
<li><p>Gridworld Example from <a class="reference internal" href="#41-policy-evaluation-prediction-problem"><span class="xref myst">section 4.1</span></a> (continue)</p>
<ul>
<li><p>After getting the value function $v_{\pi}$ of the initial random policy $\pi$, we perform greedification to get the new policy $\pi’$ (illustrated by white arrows), which is strictly better than $\pi$ according to the nature of policy improvement.</p>
  <div style="display: flex; justify-content: center;">
  <img src="../img/chapter4/example4.2.png" alt="Policy Improvement" style="width: 500px;">        
  </div>
</li>
<li><p>In doing so, we now have successfully improved our original policy (for one iteration).</p></li>
</ul>
</li>
</ul>
</section>
<section id="policy-iteration-control">
<h2>4.3 Policy Iteration (Control)<a class="headerlink" href="#policy-iteration-control" title="Link to this heading">#</a></h2>
<ul>
<li><p>$\textit{Policy iteration}$: the process of performing policy evaluation and policy improvement iteratively to find the optimal policy.</p>
<ul class="simple">
<li><p>Because a finite MDP has only a finite number of policies, this process must converge to an optimal policy and optimal value function in a finite number of iterations. With $E$ stands for evaluation and $I$ for improvement, the process is:</p></li>
</ul>
<p>$$
\pi_0 \xrightarrow{\text{E}} v_{\pi_0} \xrightarrow{\text{I}} \pi_1 \xrightarrow{\text{E}} v_{\pi_1} \xrightarrow{\text{I}} … \xrightarrow{\text{I}} \pi_{\star} \xrightarrow{\text{E}} v_{\star}
$$</p>
</li>
<li><p>Algorithm for Policy Iteration:</p>
<ul class="simple">
<li><p>Initialization $V(s) \in R$ and $\pi(s) \in A(s)$ arbitrarily for all $s \in S$</p></li>
<li><p>Policy Evaluation (see the algorithm in <a class="reference internal" href="#41-policy-evaluation-prediction-problem"><span class="xref myst">section 4.1</span></a>
)</p></li>
<li><p>Policy Improvement:</p>
<ul>
<li><p><em>policy-stable</em> $\leftarrow$ <em>true</em></p></li>
<li><p>For each $s \in S$:</p>
<ul>
<li><p><em>old-action</em> $\leftarrow \pi(s)$</p></li>
<li><p>$\pi(s) \leftarrow \arg \underset{a}{\max} \ \sum_{s’,r}\  p(s’,r|s,a) \ [r + \gamma v_{\pi}(s’)]$</p></li>
<li><p>if <em>old-action</em> $\neq \pi(s)$ and $q(s, \pi(s)) \neq q(s, \textit{old-action})$, then <em>policy-stable</em> $\leftarrow$ <em>false</em></p></li>
</ul>
</li>
<li><p>if <em>policy-stable</em>, then stop and return $V$ as $V_{\star}$ and $\pi$ as $\pi_{\star}$, else go to step 2.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>New Gridworld Example: again, if you find this all too abstract, watch this <a class="reference external" href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/Xv32P/policy-iteration">lecture video</a> which should give you a more slow and smooth illustration.</p>
<ul>
<li><p>Description:</p>
<ul>
<li><p>The Gridworld example in <a class="reference internal" href="#41-policy-evaluation-prediction-problem"><span class="xref myst">section 4.1</span></a> actually reaches the optimal policy after only one iteration, we now make the example a bit more complex by elimating one terminal grid and adding blue states where rewards have much lower value of $-10$.</p>
  <div style="display: flex; justify-content: center;">
  <img src="../img/chapter4/example4.3.1.png" alt="New Gridworld" style="width: 380px;">        
  </div>
</li>
</ul>
</li>
<li><p>Policy iteraton:</p>
<ul>
<li><p>The first iteration:</p>
  <div style="display: flex; justify-content: center; gap: 20px;">
  <div style="display: flex; flex-direction: column; align-items: center;">
  <h4>Evaluation</h4>
  <img src="../img/chapter4/example4.3.2.png" alt="Evaluation" style="width: 200px;">
  </div>
  <div style="display: flex; flex-direction: column; align-items: center;">
  <h4>Improvement</h4>
  <img src="../img/chapter4/example4.3.3.png" alt="Improvement" style="width: 200px;">
  </div>
  </div>
</li>
<li><p>The second iteration:</p>
<ul>
<li><p>Pay attention to how exactly the greedification is performed (e.g., the second left grid on the last row).</p>
<p>Remember that $\pi’(s) = \arg\max_a \sum_{s’, r} p(s’, r|s,a)[r + \gamma v_{\pi}(s’)]$ not $\pi’(s) = \arg\max_a v(s’)$, so don’t just greedify an action towards a state $s’$ simply because that state has a higher value $v(s’)$.</p>
</li>
</ul>
  <div style="display: flex; justify-content: center; gap: 20px;">
  <div style="display: flex; flex-direction: column; align-items: center;">
  <h4>Evaluation</h4>
  <img src="../img/chapter4/example4.3.4.png" alt="Evaluation" style="width: 200px;">
  </div>
  <div style="display: flex; flex-direction: column; align-items: center;">
  <h4>Improvement</h4>
  <img src="../img/chapter4/example4.3.5.png" alt="Improvement" style="width: 200px;">
  </div>
  </div>
</li>
<li><p>…</p></li>
<li><p>The final iteration</p>
<ul class="simple">
<li><p>Note that in the final iteration, after policy improvement, the policy remains the same, i.e., the original policy (before improvement) is already the greedy policy with respect to its own value function, the optimal policy is found.</p></li>
</ul>
  <div style="display: flex; justify-content: center; gap: 20px;">
  <div style="display: flex; flex-direction: column; align-items: center;">
  <h4>Evaluation</h4>
  <img src="../img/chapter4/example4.3.6.png" alt="Evaluation" style="width: 200px;">
  </div>
  <div style="display: flex; flex-direction: column; align-items: center;">
  <h4>Improvement</h4>
  <img src="../img/chapter4/example4.3.7.png" alt="Improvement" style="width: 200px;">
  </div>
  </div>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="value-iteration">
<h2>4.4 Value Iteration<a class="headerlink" href="#value-iteration" title="Link to this heading">#</a></h2>
<ul>
<li><p>Value Iteration:  is the special case of Policy Iteration where policy evaluation stops after just one sweep.</p>
<ul class="simple">
<li><p>Drawback of policy iteration: requires iterative computation, each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set, and convergence only occurs in the limit, which takes a lot of time.</p></li>
<li><p>Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement</p></li>
</ul>
</li>
<li><p>Update rule:</p>
<p>$$
\begin{align*}
V_{k+1}(s) \ &amp;\dot= \ \max_a q_{k}(s, a) \
&amp;= \max_a \sum_{s’, r} p(s’, r|s,a) [r + \gamma V_{k}(s’)]\ \text{for all} s \in S
\end{align*}
$$</p>
<ul class="simple">
<li><p>Note that the above equation is obtained simply by turning Bellman Optimality Equation into an updating rule.</p></li>
</ul>
</li>
<li><p>Algorithm for Value Iteration:</p>
<ul class="simple">
<li><p>Initialize a small threshold $\theta &gt; 0$ for determining accuracy of estimation.</p></li>
<li><p>Initialize $V(s)$, for all $s \in S^+$, arbitrarily except that $V(terminal) = 0$.</p></li>
<li><p>Loop:</p>
<ul>
<li><p>$\Delta \leftarrow 0$</p></li>
<li><p>For $s \in S$:</p>
<ul>
<li><p>$v \leftarrow V(s)$</p></li>
<li><p>$V(s) = \underset{a}{\max} \underset{s’, r}{\sum} p(s’, r|s,a) [r + \gamma V(s’)]$</p></li>
<li><p>$\Delta \leftarrow \text{max}(\Delta, |v - V(s)|)$</p></li>
</ul>
</li>
<li><p>until $\Delta &lt; \theta$</p></li>
</ul>
</li>
<li><p>Output a deterministic policy, $\pi \approx \pi_{\star}$, such that
$$\pi(s) = \arg \underset{a}{\max}  \underset{s’, r}{\sum} p(s’, r|s,a) [r + \gamma V(s’)]$$</p></li>
</ul>
</li>
</ul>
</section>
<section id="generalized-policy-iteration-gpi">
<h2>4.5 Generalized Policy Iteration (GPI)<a class="headerlink" href="#generalized-policy-iteration-gpi" title="Link to this heading">#</a></h2>
<ul>
<li><p>$\star$ Two types of DP:</p>
<ul class="simple">
<li><p>Synchronous DP: update all states systematically in a certain order (takes very long for large state space)</p></li>
<li><p>Asynchronous DP: update states without order (can be faster, but also problematic when only a small set of states being updated constantly)</p></li>
</ul>
</li>
<li><p>$\textit{Generalized policy iteration (GPI)}$: refers to the general idea of letting policy-evaluation and policyimprovement processes interact, independent of the granularity and other details of the two processes.</p>
<ul>
<li><p>Almost all reinforcement learning methods are well described as GPI. That is, all have identifiable policies and value functions, with the policy always being improved with respect to the value function and the value function always being driven toward the value function for the policy:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../img/chapter4/gpi2.png" alt="Generalized policy iteration" style="width: 280px;">
  </div>
<p>Both processes stabilize only when a policy has been found that is greedy with respect to its own evaluation function (when $\pi$ itself is the greedy policy of $v_{\pi}$). This implies that the Bellman optimality equation for state-value function holds, and thus that the policy and the value function are optimal.</p>
</li>
</ul>
</li>
</ul>
</section>
<section id="summary">
<h2>4.6 Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>Classical DP methods operate in sweeps through the state set, performing an $\textit{expected update}$ operation on each state. The update of states based on estimates of the values of successor states. That is, estimates are updated on the basis of other estimates. We call this general idea $\textit{bootstrapping}$ (a very fundamental concept in many RL algorithms, and we will introduce it in depth in <a class="reference internal" href="6_temporal_difference_learning.html"><span class="std std-doc">Chapter 6</span></a>) and requires a perfect model of the environment.</p>
<p>In the next chapter we explore Monte Carlo method - a reinforcement learning method that do not require a model and do not bootstrap. But for now, a quick summary:</p>
<ul class="simple">
<li><p>Mindmap of where we are now
<img src="../img/chapter4/chapter4_mindmap.png" alt="Mindmap" style="width:100%;"></p></li>
<li><p>Key Takeaways:</p>
<ol class="arabic simple">
<li><p>DP Overview:</p>
<ul>
<li><p>Computes optimal policies using a perfect model of the environment (MDP).</p></li>
<li><p>No interaction with the environment; computationally expensive but foundational for RL.</p></li>
<li><p>Core idea is to use value functions and Bellman equations as update rules to improve policies.</p></li>
</ul>
</li>
<li><p>Policy Evaluation (Prediction):</p>
<ul>
<li><p>Iteratively estimate $v_\pi$ for a given policy $\pi$:
$$
v_{k+1}(s) = \sum_a \pi(a|s) \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_k(s’)]
$$</p></li>
<li><p>Stop when changes in $v(s)$ are smaller than a threshold $\theta$.</p></li>
</ul>
</li>
<li><p>Policy Improvement:</p>
<ul>
<li><p>Create a greedy policy $\pi’$ with respect to $v_\pi$:
$$
\pi’(s) = \arg\max_a \sum_{s’, r} p(s’, r | s, a) [r + \gamma v_\pi(s’)]
$$</p></li>
<li><p>If no further improvement is possible, $\pi$ is optimal.</p></li>
</ul>
</li>
<li><p>Policy Iteration:</p>
<ul>
<li><p>Alternate between policy evaluation and improvement until convergence:
$$
\pi_0 \rightarrow v_{\pi_0} \rightarrow \pi_1 \rightarrow v_{\pi_1} \rightarrow \ldots \rightarrow \pi_* \rightarrow v_*
$$</p></li>
</ul>
</li>
<li><p>Value Iteration:</p>
<ul>
<li><p>Simplified policy iteration with one sweep per iteration:
$$
V_{k+1}(s) = \max_a \sum_{s’, r} p(s’, r | s, a) [r + \gamma V_k(s’)]
$$</p></li>
<li><p>Converges to optimal $V$ and greedy policy $\pi_*$.</p></li>
</ul>
</li>
<li><p>Generalized Policy Iteration (GPI):</p>
<ul>
<li><p>Continuous interaction of policy evaluation and improvement.</p></li>
<li><p>Stabilizes when the policy is greedy with respect to its own value function.</p></li>
</ul>
</li>
</ol>
</li>
<li><p>Optional lecture video: <a class="reference external" href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/phTdz/warren-powell-approximate-dynamic-programming-for-fleet-management-long">Warren Powell: Approximate Dynamic Programming for Fleet Management (Long)</a></p></li>
</ul>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="3_markov_decision_process.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 3. Finite Markov Decision Processes</p>
      </div>
    </a>
    <a class="right-next"
       href="5_monte_carlo_methods.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 5. Monte Carlo Methods</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-evaluation-prediction-problem">4.1 Policy Evaluation (Prediction problem)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-improvement">4.2 Policy Improvement</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-iteration-control">4.3 Policy Iteration (Control)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#value-iteration">4.4 Value Iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generalized-policy-iteration-gpi">4.5 Generalized Policy Iteration (GPI)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">4.6 Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Youxiang Dong
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025, Youxiang Dong.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>