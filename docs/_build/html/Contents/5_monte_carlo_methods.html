
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 5. Monte Carlo Methods &#8212; DistilRLIntro 0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=e1a75a79"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Contents/5_monte_carlo_methods';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 6. Temporal-Difference Learning" href="6_temporal_difference_learning.html" />
    <link rel="prev" title="Chapter 4. Dynamic Programming" href="4_dynamic_programming.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">DistilRLIntro 0.1 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="0_prelude.html">Why this project</a></li>



<li class="toctree-l1"><a class="reference internal" href="1_intro.html">Chapter 1. Introduction to RL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Fundamentals of Reinforcement Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2_multi_armed_bandits.html">Chapte 2. Multi-armed Bandit</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_markov_decision_process.html">Chapter 3. Finite Markov Decision Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_dynamic_programming.html">Chapter 4. Dynamic Programming</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Sample-based Learning Methods</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 5. Monte Carlo Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="6_temporal_difference_learning.html">Chapter 6. Temporal-Difference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="7_planning_learning_acting.html">Chapter 7. Planning and Learning with Tabular Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Value Function Approximation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="8_on_policy_prediction_with_approximation.html">Chapter 8. On-policy Prediction with Approximation</a></li>
<li class="toctree-l1"><a class="reference internal" href="9_on_policy_control_with_approximation.html">Chapter 9. On-policy Control with Approximation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Policy Approximation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_policy_gradient_methods.html">Chapter 10. Policy Gradient Methods</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Contents/5_monte_carlo_methods.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 5. Monte Carlo Methods</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-prediction-evaluation">5.1 Monte Carlo Prediction (Evaluation)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mc-prediction-for-state-value-function">5.1.1 MC Prediction for state-value function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mc-prediction-for-action-value-function">5.1.2 MC Prediction for action-value function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-control">5.2 Monte Carlo Control</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-control-removing-assumption-2">5.2.1 Monte Carlo Control removing Assumption (2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-control-removing-both-assumptions">5.2.2 Monte Carlo Control removing both assumptions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#off-policy-monte-carlo-methods">5.3 Off-policy Monte Carlo Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#off-policy-monte-carlo-prediction-via-importance-sampling">5.3.1 Off-policy Monte Carlo Prediction via Importance Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#off-policy-monte-carlo-control">5.3.2 Off-policy Monte Carlo Control</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">5.4 Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="chapter-5-monte-carlo-methods">
<h1>Chapter 5. Monte Carlo Methods<a class="headerlink" href="#chapter-5-monte-carlo-methods" title="Link to this heading">#</a></h1>
<p>Monte Carlo methods are ways of solving the reinforcement learning problem based on <strong>experience only</strong>, i.e., averaging sample returns. To ensure that well-defined returns are available, here we define Monte Carlo methods only for <strong>episodic tasks</strong>.</p>
<p>Only on the completion of an episode are value estimates and policies changed. Monte Carlo methods can thus be incremental in an <strong>episode-by-episode</strong> sense, but not in a step-by-step (online) sense.</p>
<p>We adapt the idea of general policy iteration (GPI) and learn value functions from sample returns with the MDP. The value functions and corresponding policies still interact to attain optimality in essentially the same way (GPI).</p>
<p>One final note about the notation: <strong>throughout this chapter, we assume an episode always starts at time step $0$ and ends at time step $T$, i.e., $S_T$ is the terminal state.</strong></p>
<section id="monte-carlo-prediction-evaluation">
<h2>5.1 Monte Carlo Prediction (Evaluation)<a class="headerlink" href="#monte-carlo-prediction-evaluation" title="Link to this heading">#</a></h2>
<p>Basics of MC methods:</p>
<ul class="simple">
<li><p>Define a $\textit{visit}$ to state $s$: an occurrence of state s in an episode. Of course, $s$ may be visited multiple times in the same episode</p></li>
<li><p>Evaluation methods:</p>
<ul>
<li><p><strong>first-visit MC method</strong> estimates $v_{\pi}(s)$ as the average of the returns following first visits to $s$.</p></li>
<li><p><strong>every-visit MC method</strong> averages the returns following all visits to $s$.</p></li>
</ul>
</li>
</ul>
<section id="mc-prediction-for-state-value-function">
<h3>5.1.1 MC Prediction for state-value function<a class="headerlink" href="#mc-prediction-for-state-value-function" title="Link to this heading">#</a></h3>
<ul>
<li><p>First-visit MC prediction, for estimating $V \approx v_{\pi}$</p>
<ul>
<li><p>Algorithm:</p>
<ul class="simple">
<li><p>Input: a policy $\pi$ to be evaluated</p></li>
<li><p>Initialize:</p>
<ul>
<li><p>$V(s) \in \mathbb{R}$ arbitrarily, for all $s \in S$</p></li>
<li><p>$Return(s) \leftarrow$ an empty list for all $s \in S$</p></li>
</ul>
</li>
<li><p>Loop forever (for each episode):</p>
<ul>
<li><p>Generate an episode following $\pi: S_0, A_0, R_1, S_1, A_1, …, S_{T-1}, A_{T-1}, R_T, S_T$</p></li>
<li><p>$G_T \leftarrow 0$</p></li>
<li><p>$ \text{for } t \text{ in } {T-1, T-2, …, 0}$:</p>
<ul>
<li><p>$G_{t} \leftarrow \gamma G_{t+1} + R_{t+1}$</p></li>
<li><p>Append $G_{t}$ to $Returns(S_t)$</p></li>
<li><p>$V(S_t) \leftarrow average(Returns(S_t))$</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Intuition: the realizations of return $G_t$ for each state is calculated backwards from $S_{T-1}$ to $S_0$, by the law of large number, the average of $G_t$ for each state will be the value of that state: $v(S_t) = E_{\pi}[G_t|S_t]$</p></li>
<li><p>Visualization of return calculation ($T=5$):</p>
  <div style="display: flex; justify-content: center;">
  <img src="../img/chapter5/computing_gt.png" alt="Backward calculation of returns" style="width: 350px;">
  </div>
</li>
</ul>
</li>
<li><p>Notes:</p>
<ul class="simple">
<li><p>An important fact about MC method is that the estimates for each state are <strong>independent.</strong> The estimate for one state does not build upon the estimate of any other state, as is the case in DP. In other words, Monte Carlo methods do not bootstrap as we defined it in the previous chapter.</p></li>
<li><p>For MC method, the computational expense of estimating the value of a single state is independent of the number of states. This can make Monte Carlo methods particularly attractive when one requires the value of only one or a subset of states.</p></li>
</ul>
</li>
</ul>
</section>
<section id="mc-prediction-for-action-value-function">
<h3>5.1.2 MC Prediction for action-value function<a class="headerlink" href="#mc-prediction-for-action-value-function" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Motivation: state values $v_{\pi}$ are only usable when we have the model of the environment. Since MC methods assume there is <strong>no model available</strong>, one of our primary goals in this case is to actually estimate $q_\star$.</p></li>
<li><p>First-visit MC prediction, for estimating $Q \approx q_{\pi}$</p>
<ul>
<li><p>Algorithm:</p>
<ul>
<li><p>Input: a policy $\pi$ to be evaluated</p></li>
<li><p>Initialize:</p>
<ul>
<li><p>$Q(s, a) \in \mathbb{R}$ arbitrarily, for all $s \in S, a \in A(s)$.</p></li>
<li><p>$Return(s, a) \leftarrow$ an empty list for all $s \in S, a \in A(s)$.</p></li>
</ul>
</li>
<li><p>Loop forever (for each episode):</p>
<ul>
<li><p>Generate an episode following $\pi: S_0, A_0, R_1, S_1, A_1, …, S_{T-1}, A_{T-1}, R_T, S_T$</p></li>
<li><p>$G_T \leftarrow 0$</p></li>
<li><p>$ \text{for } t \text{ in } {T-1, T-2, …, 0}$:</p>
<ul>
<li><p>$G_{t} \leftarrow \gamma G_{t+1} + R_{t+1}$</p></li>
<li><p>Append $G_{t}$ to $Returns(S_t, A_t)$</p></li>
<li><p>$q(S_t, A_t) \leftarrow average(Returns(S_t, A_t))$</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Intuition: same as in <a class="reference internal" href="#511-mc-prediction-for-state-value-function"><span class="xref myst">section 5.1.1</span></a>, remember that $q(s,a) = E_\pi[G_t | S_t, A_t]$</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="monte-carlo-control">
<h2>5.2 Monte Carlo Control<a class="headerlink" href="#monte-carlo-control" title="Link to this heading">#</a></h2>
<p>General problems and two basic assumptions we rely on in <a class="reference internal" href="#51-monte-carlo-prediction-evaluation"><span class="xref myst">section 5.1</span></a>:</p>
<ul class="simple">
<li><p>Problem of $\textit{maintaining exploration}$: in estimating $q_{\pi}$, many state-action pairs may never be visited. E.g., if the policy is deterministic, many actions at a state may not be taken.</p>
<ul>
<li><p><strong>Assumption (1)</strong> of $\textit{exploring starts}$: episodes start in a state-action pair, and every pair has a nonzero probability of being selected as the start. (So every state-action pair will be visited an infinite number of times in the limit of an infinite number of episodes.)</p></li>
</ul>
</li>
<li><p>Problem of estimating $\hat{q}_{\pi}(S_t, A_t)$: by default, we used the law of large number and rely on the following assumption:</p>
<ul>
<li><p><strong>Assumption (2)</strong> of infinite number of episodes: policy evaluation can be done with infinite number of episodes (complete policy evaluation).</p></li>
</ul>
</li>
</ul>
<p>Apparently, these two assumptions are hardly truth in practice, so we are now going to introduce methods that remove them gradually.</p>
<section id="monte-carlo-control-removing-assumption-2">
<h3>5.2.1 Monte Carlo Control removing Assumption (2)<a class="headerlink" href="#monte-carlo-control-removing-assumption-2" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>How to <strong>remove Assumption (2)</strong>:</p>
<ul>
<li><p>To avoid infinite number of episodes nominally required for policy evaluation, we could <strong>give up trying to complete policy evaluation</strong> before returning to policy improvement. Value iteration can be seen as an extrem example of this idea.</p></li>
<li><p>For Monte Carlo policy iteration it is natural to alternate between evaluation and improvement on an <strong>episode-by-episode</strong> basis. After each episode, the observed returns are used for policy evaluation, and then the policy is improved at all the states visited in the episode.</p></li>
</ul>
</li>
<li><p>Monte Carlo ES (Exploring Starts), for estimating $\pi \approx \pi_{\star}$</p>
<ul>
<li><p>Algorithm:</p>
<ul>
<li><p>Initialize:</p>
<ul>
<li><p>$\pi(s) \in A(s)$ (arbitrarily), for all $s \in S$</p></li>
<li><p>$Q(s,a) \in \mathbb{R}$ (arbitrarily), for all $s \in S$</p></li>
<li><p>$Returns(s, a) \leftarrow$ empty list, for all $s \in S, a \in A(s)$</p></li>
</ul>
</li>
<li><p>Loop forever (for each episode):</p>
<ul>
<li><p><strong>Choose $S_0 \in S, A_0 \in A(S_0)$ randomly so that all state-action pairs have probability $&gt; 0$ (Exploring Start).</strong></p></li>
<li><p>Generate an episode from the chosen start following $\pi(s)$, as $S_0, A_0, R_1, S_1, …, S_{T-1}, A_{T-1}, R_T, S_T$</p></li>
<li><p>$G_T \leftarrow 0$</p></li>
<li><p>Loop for each step of episode: $t=T-1, T-2, …, 0$:</p>
<ul>
<li><p>$G_t \leftarrow \gamma G_{t+1} + R_{t+1}$</p></li>
<li><p>$ \text{for } t \text{ in } {T-1, T-2, …, 0}$:</p>
<ul>
<li><p>Append $G_t$ to $Returns(S_t, A_t)$</p></li>
<li><p>$Q(S_t, A_t) \leftarrow average(Returns(S_t, A_t))$</p></li>
<li><p><strong>$\pi(S_t) \leftarrow \arg \underset{a}{\max}(Q(S_t, A_t))$ (removing the second assumption)</strong></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Notes:</p>
<ul>
<li><p>The essential technique of above algorithm is that after each update of $Q(S_t,A_t)$, the improvement (greedification) will be made directly.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="monte-carlo-control-removing-both-assumptions">
<h3>5.2.2 Monte Carlo Control removing both assumptions<a class="headerlink" href="#monte-carlo-control-removing-both-assumptions" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>On / Off-policy methods:</p>
<ul>
<li><p>On policy methods: attempt to evaluate or improve the policy that is used to make decisions. (e.g., MC with ES, dynamic programming etc.)</p></li>
<li><p>Off policy methods: evaluate or improve a policy different from that used to generate the data.</p></li>
</ul>
</li>
<li><p>$\epsilon$-soft policies:</p>
<ul>
<li><p>$\epsilon$-greedy policy: as introduced in Chapter 2 <a class="reference internal" href="#(../Contents/2_multi_armed_bandits.md#22-action-value-methods)"><span class="xref myst">section 2.2</span></a> all non-greedy action are given the minimal probability of selection $\frac{\epsilon}{|A(s)|}$ (<strong>uniform distribution</strong>), the greedy action has the probability of $1 - \epsilon + \frac{\epsilon}{|A(s)|}$.</p>
<ul>
<li><p>$\epsilon$-greedy policy is a type of $\epsilon$-soft policies. Among $\epsilon$-soft policies, $\epsilon$-greedy policies are in some sense those that are closest to greedy.</p></li>
</ul>
</li>
<li><p>$\epsilon$-soft policy: all actions have probability of $\pi(a|s)&gt;\frac{\epsilon}{|A(s)|}$ for all states. This means that the agent explores all possible actions with non-zero probability $\frac{\epsilon}{|A(s)|}$, but <strong>not necessarily uniformly</strong>.</p></li>
</ul>
</li>
<li><p>On-Policy first-visit Monte Carlo Control (with $\epsilon$-soft policy), for estimating $\pi \approx \pi_{\star}$</p>
<ul>
<li><p>Algorithm:</p>
<ul>
<li><p>Algorithm parameter: small $\epsilon &gt; 0$</p></li>
<li><p>Initialize:</p>
<ul>
<li><p><strong>$\pi(s) \leftarrow$ an arbitrary $\epsilon$-soft policy (removing the first assumption)</strong></p></li>
<li><p>$Q(s,a) \in \mathbb{R}$ (arbitrarily), for all $s \in S, a \in A(s)$</p></li>
<li><p>$Returns(s, a) \leftarrow$ empty list, for all $s \in S, a \in A(s)$</p></li>
</ul>
</li>
<li><p>Loop forever (for each episode):</p>
<ul>
<li><p>Generate an episode following $\pi: S_0, A_0, R_1, … S_{T-1}, A_{T-1}, R_T, S_T$</p></li>
<li><p>$G_T \leftarrow 0$</p></li>
<li><p>Loop for each step of episode: $t=T-1, T-2, …, 0$:</p>
<ul>
<li><p>$G_t \leftarrow \gamma G_{t+1} + R_{t+1}$</p></li>
<li><p>Unless the pair $S_t, A_t$ appears in $S_0, A_0, S_1, …, S_{T-1}, A_{T-1}$</p>
<ul>
<li><p>Append $G_t$ to $Returns(S_t, A_t)$</p></li>
<li><p>$Q(S_t, A_t) \leftarrow average(Returns(S_t, A_t))$</p></li>
<li><p>$A^* \leftarrow \arg \underset{a}{\max} \ Q(S_t, A_t)$ (with ties broken arbitrarily)</p></li>
<li><p>For all $a \in A(S_t)$:
$$
\colorbox{lightyellow}{$
\pi(S_t) =
\left {
\begin{array}{ll}
1 - \epsilon + \frac{\epsilon}{|A(s_t)|} &amp; \text{if } a = A^* \
\frac{\epsilon}{|A(s_t)|} &amp; \text{if } a \neq A^*
\end{array}
\right.
$}
$$</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Intuition: If we remove the assumption of exploring starts, we cannot simply improve the policy by making it greedy with respect to the current value function, because that would prevent further exploration of nongreedy actions. So on the basis of Monte Carlo ES (Exploring Starts) in <a class="reference internal" href="#521-monte-carlo-control-removing-assumption-2"><span class="xref myst">section 5.2.1</span></a>, this algorithm removes the assumptions of exploring start by:</p>
<ul>
<li><p>defining the initial policy to be $\epsilon$-soft and,</p></li>
<li><p>updating the old policy to be $\epsilon$-greedy policy during policy improvement.</p></li>
</ul>
</li>
<li><p>Notes:</p>
<ul>
<li><p>The algorithm above also removes the second assumption (infinite episode) because the $\epsilon$-greedification happens immediately after the calculation of the $Q(S_t, A_t)$.</p></li>
<li><p>Policy improvement theorem assures that any $\epsilon$-greedy policy with respect to $q_\pi$ is an improvement over any $\epsilon$-soft policy.</p></li>
<li><p>Note that we now only achieve the best policy among the $\epsilon$-soft policies, i.e, a near-optimal policy which still explores, not really the optimal policy.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section id="off-policy-monte-carlo-methods">
<h2>5.3 Off-policy Monte Carlo Methods<a class="headerlink" href="#off-policy-monte-carlo-methods" title="Link to this heading">#</a></h2>
<p>All learning control methods face a dilemma: They seek to learn action values conditional on subsequent optimal behavior, but they need to behave non-optimally in order to explore all actions (to find the optimal actions). How can they learn about the optimal policy while behaving according to an exploratory policy? A more straightforward approach is to use two policies</p>
<p>Let’s recap On / Off-policy learning:</p>
<ul class="simple">
<li><p>On-policy learning: learns the value or policy function for the current $\textit{target policy}$ $\pi$ that the agent is following. This means that the agent learns by interacting with the environment using the same policy that it is improving. <strong>On-policy methods are generally simpler and are considered first.</strong></p></li>
<li><p>Off-policy learning: the learning is from data generated by $\textit{behavior policy}$ $b$ and is “off” the $\textit{target policy}$ $\pi$. Off-policy methods require additional concepts and notation, and because the data is due to a different policy, <strong>off-policy methods are often of greater variance and are slower to converge.</strong> <br />
<strong>On the other hand, off-policy methods are more powerful and general.</strong> They include on-policy methods as the special case in which the target and behavior policies are the same.</p>
<ul>
<li><p>In order to use episodes from $b$ to estimate values for $\pi$, we proceede based on the <strong>assumption of coverage</strong>: wherever $\pi(a|s) \ge 0$, $b(a|s) \ge 0$ must also hold. This means, behavior policy $b$ must be stohastic in states where it is not identical to the target policy $\pi$.</p></li>
</ul>
</li>
</ul>
<section id="off-policy-monte-carlo-prediction-via-importance-sampling">
<h3>5.3.1 Off-policy Monte Carlo Prediction via Importance Sampling<a class="headerlink" href="#off-policy-monte-carlo-prediction-via-importance-sampling" title="Link to this heading">#</a></h3>
<ul>
<li><p>$\textit{Importance Sampling}$ Implementation: Here is a <a class="reference external" href="https://www.coursera.org/learn/sample-based-learning-methods/lecture/XPxPd/importance-sampling">lecture video</a> to explain importance sampling should you find the textual derivation hard to understand.</p>
<ul class="simple">
<li><p>given existing samples from $X \sim b$, we want to estimate $E_{\pi}[X]$ (of a different distribution)</p></li>
<li><p>derivation:
$$
\begin{align*}
E_{\pi}[X] &amp;= \sum_{x \in X} x \pi(x) \
&amp;= \sum_{x \in X} x \pi(x) \frac{b(x)}{b(x)} \
&amp;= \sum_{x \in X} x b(x) \frac{\pi(x)}{b(x)} \
&amp;= \sum_{x \in X} x b(x) \rho(x)  \quad (\rho(x) \text{ denotes } \frac{\pi(x)}{b(x)}) \
&amp;= E_{b}[X \rho(X)] \
&amp;\approx \frac{1}{n} \sum_{i=1}^n x_i \rho(x_i)
\end{align*}
$$</p></li>
<li><p>the ratio $\rho(x)=\frac{\pi(x)}{b(x)}$ is called the <strong>importance sampling ratio</strong>.</p></li>
</ul>
</li>
<li><p>Importance Sampling for evaluating the target policy $\pi$ <strong>in theory</strong>:</p>
<ol class="arabic">
<li><p>Calculating the importance sampling ratio $\frac{\pi}{b}$ <strong>for one given state-action trajectory</strong></p>
<ul>
<li><p>Given a trajectory $S_{t+1}, A_{t+1}, S_{t+2}…, A_{T-1}, S_T$, starting from $S_t, A_t$, the probability of this trajectory is</p>
<p>$$
\begin{align*}
Pr (&amp;S_{t+1}, A_{t+1} ,…, S_{T-1}, A_{T-1}, S_T | S_t, A_t) \
&amp;= p(S_{t+1}|A_t, S_t)\pi(A_{t+1}|S_{t+1}) … p(S_{T-1}|A_{T-2}, S_{T-2})\pi(A_{T-1}|S_{T-1})p(S_T|A_{T-1},S_{T-1}) \
&amp;= \Pi_{k=t+1}^{k={T-1}} p(S_k|A_{k-1}, S_{k-1})\pi(A_k|S_k) \times p(S_T|A_{T-1},S_{T-1})
\end{align*}
$$<br />
Therefore the <strong>importance sampling ratio</strong>:
$$
\begin{align*}
\rho_{t+1: T-1} &amp;= \frac{\Pi_{k=t+1}^{k={T-1}} p(S_k|A_{k-1}, S_{k-1})\pi(A_k|S_k) \times p(S_T|A_{T-1},S_{T-1})}{\Pi_{k=t+1}^{k={T-1}} p(S_k|A_{k-1}, S_{k-1})b(A_k|S_k) \times p(S_T|A_{T-1},S_{T-1})} \
&amp;= \frac{\Pi_{k={t+1}}^{k={T-1}} \pi(A_k|S_k)}{\Pi_{k={t+1}}^{k={T-1}} b(A_k|S_k)}
\end{align*}
$$</p>
</li>
<li><p>Notes</p>
<ul class="simple">
<li><p>the subscript of $\rho_{t: T-1}$ corresponds to the sequence of actions in the trajectory, i.e., {$A_{t+1}, …, A_{T-1}$}, then the trajectory stops at the terminal state $S_T$</p></li>
<li><p>$\rho_{t+1: T-1}$ only depends on the two policies, <strong>not the dynamics of the environment</strong>, which means importance sampling can be used in model-free RL problems.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Estimating $q_{\pi}(s,a)$ given $q_b(s,a) = E_b[G_t|S_t = s, A_t = a]$ as:
$$q_{\pi}(s,a)= E_b[\rho_{t+1:T-1} \times G_t|S_t = s, A_t=a]$$</p>
<ul class="simple">
<li><p>Intuition: note that the value function for $q$: $E[G_t|S_t = s, A_t=a]$ is caculating an expection based on all given trajectories, so every $G_t$ is a single realisation, which can be seen as the variable $x$ in the derivation of importance sampling equation. <br />
The importance sampling ratio $\rho_{t+1:T-1}$ is caculated on a trajectory basis to correspond to this trajectory-based charactor of $G_t$ (and is used for multiplication with $G_t$ directly).</p></li>
</ul>
</li>
</ol>
</li>
<li><p>Importance sampling for evaluating the target policy $\pi$ <strong>in practice</strong></p>
<ul>
<li><p>Settings:</p>
<ul class="simple">
<li><p>About time steps: the time steps will be numbered in a way that increases across episode boundaries for convenience. That is, if the first episode of the batch ends in a terminal state at $t=100$, then the next episode begins at $t = 101$.</p></li>
<li><p>About notations:</p>
<ul>
<li><p>$J(s,a)$: <strong>the set of time steps</strong> in which state action pair $(s,a)$ is visited (This is for an every-visit method; for a first-visit method, $J(s,a)$ would only include time steps that were first visits to $(s,a)$ within their episodes).</p></li>
<li><p>$T(t)$: <strong>the time step</strong> of the first terminal state from time step $t$.</p></li>
<li><p>${G_t}_{t \in J(s,a)}$: the set of returns that pertain to state action pair $(s,a)$ from all episodes.</p></li>
<li><p>${\rho_{t+1:T(t)-1}}<em>{t \in J(s,a)}$: the importance sampling ratio for the trajectory ${S</em>{t+1}, A_{t+1}, …, S_{T(t)-1}, A_{T(t)-1}, S_{T(t)} }$.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Approaches:</p>
<ul>
<li><p>Ordinary importance sampling for evaluating target policy:
$$
Q(s,a) \dot= \frac{\sum_{t \in J(s)} \ \rho_{t+1:T(t)-1} \times G_t}{|J(s,a)|}
$$</p></li>
<li><p>Weighted importance sampling for evaluating target policy:
$$
Q(s,a) \dot= \frac{\sum_{t \in J(s)} \ \rho_{t+1:T(t)-1} \times G_t}{\sum_{t \in J(s)} \ \rho_{t+1:T(t)-1}}
$$</p></li>
<li><p>Notes:</p>
<ul>
<li><p>for first-visit methods:</p>
<p>Consider the estimates of their first-visit methods after observing a single return $G_t$ from $(s,a)$, in the weighted-average estimate, the estimate is equal to the observed return $G_t$ independent of the ratio, its estimate in this case is $q_b(s,a)$ rather than $q_\pi(s,a)$. In contrast, the estimate from ordinary method is always $q_\pi(s,a)$, but it could be highly volatile depending on the value of $\rho_{t+1:T(t)-1}$.</p>
<p>In summary, ordinary importance sampling is unbiased whereas weighted importance sampling is biased (though the bias converges asymptotically to zero). On the other hand, the variance of ordinary importance sampling is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. In fact, assuming bounded returns, the variance of the weighted importance-sampling estimator converges to zero even if the variance of the ratios themselves is infinite.</p>
<p><strong>In practice, the weighted estimator usually has dramatically lower variance and is strongly preferred.</strong> Nevertheless, we will not totally abandon ordinary importance sampling as it is easier to extend to the approximate methods using function approximation that we explore later.</p>
</li>
<li><p>for every-visit methods:</p>
<p>The every-visit methods for ordinary and weighed importance sampling are both biased, though, again, the bias falls asymptotically to zero as the number of samples increases. <strong>In practice, every-visit methods are often preferred because they remove the need to keep track of which states have been visited and because they are much easier to extend to approximations.</strong></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Incremental Implementation for updating $Q(s,a)$:</p>
<ul>
<li><p>For on-policy methods:
$Q(s,a)$ is calculated by simply averaging the collected return realizations, so the incremental implementation can be done the same way as in Chapter 2 in <a class="reference internal" href="2_multi_armed_bandits.html#241-stationary-problems"><span class="std std-ref">section 2.4.1</span></a>, namely:
$$
NewEstimate \leftarrow OldEstimate + StepSize*[Target - OldEstimate]$$</p></li>
<li><p>For off-policy methods:</p>
<ul>
<li><p>Ordinary importance sampling: the returns are also simply averaged by $J(s,a)$, so the incremental rule is the same as on-policy methods, as shown above.</p></li>
<li><p>Weighted importance sampling: here we have to form a weighted average of the returns using a slightly different incremental algorithm.</p>
<ul>
<li><p>Assume the set ${G_t}<em>{t \in J(s,a)}$ alredy contains $n-1$ items, numbered as $G_1, G_2, …, G</em>{n-1}$, and the respective weight for $G_i$ is $W_i = \rho_{i+1:T(i)-1}$ and $W_i \in {\rho_{t+1:T(t)-1}}_{t \in J(s,a)}$</p></li>
<li><p>So the $n$-th weighted average estimate for $Q(s,a)$ is:
$$
\begin{align*}
Q_n(s,a) &amp;\dot= \frac{\sum_{k=1}^{k=n-1}W_k G_k}{\sum_{k=1}^{k=n-1}W_k} \
&amp;\dot= Q_{n-1}(s,a) + \alpha \times [G_n - Q_{n-1}(s,a)]
\end{align*}
$$</p>
<p>with $\alpha = \frac{W_n}{C_n}$ and $C_n \dot= C_{n-1} + W_n = \sum_{i=1}^{i=n} W_i$, where $C_0 \dot= 0$</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Off-policy MC prediction (policy evaluation) for estimating $Q \approx q_\pi$</p>
<ul class="simple">
<li><p>Algorithm:</p>
<ul>
<li><p>Input: an arbitrary target policy $\pi$</p></li>
<li><p>Initialize, for all $s \in S$, $a \in A(s)$:</p>
<ul>
<li><p>$Q(s, a) \in \mathbb{R}$ (arbitrarily)</p></li>
<li><p>$C(s, a) \leftarrow 0$</p></li>
</ul>
</li>
<li><p>Loop forever (for each episode):</p>
<ul>
<li><p>$b \leftarrow$ any policy with coverage of $\pi$</p></li>
<li><p>Generate an episode following $b$: $S_0, A_0, R_1, \ldots, S_{T-1}, A_{T-1}, R_T, S_T$</p></li>
<li><p>$G_T \leftarrow 0$</p></li>
<li><p>$W_T \leftarrow 1$</p></li>
<li><p>Loop for each step of episode, $t = T-1, T-2, \ldots, 0,$ while $W \neq 0$:</p>
<ul>
<li><p>$G_t \leftarrow \gamma G_{t+1} + R_{t+1}$</p></li>
<li><p>$C(S_t, A_t) \leftarrow C(S_t, A_t) + W_{t+1}$</p></li>
<li><p>$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{W_{t+1}}{C(S_t, A_t)} [G_t - Q(S_t, A_t)]$</p></li>
<li><p>$W_t \leftarrow W_{t+1} \frac{\pi(A_t|S_t)}{b(A_t|S_t)}$</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Note that $W_{t+1} = \rho_{t+1:T(i)-1}$</p></li>
</ul>
</li>
</ul>
</section>
<section id="off-policy-monte-carlo-control">
<h3>5.3.2 Off-policy Monte Carlo Control<a class="headerlink" href="#off-policy-monte-carlo-control" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>In Control, the target policy is typically the deterministic greedy policy with respect to the current estimate of the action-value function. This policy becomes as deterministic optimal policy while the behavior policy remains stochastic and more exploratory, for example, an $\epsilon$-greedy policy.</p></li>
<li><p>Off-policy MC prediction (policy evaluation) for estimating $\pi \approx \pi_\star$</p>
<ul>
<li><p>Algorithm:</p>
<ul>
<li><p>Initialize, for all $s \in S$, $a \in A(s)$:</p>
<ul>
<li><p>$Q(s, a) \in \mathbb{R}$ (arbitrarily)</p></li>
<li><p>$C(s, a) \leftarrow 0$</p></li>
<li><p>$\pi(s) \leftarrow \arg \max_a Q(s,a)$ (with ties broken consistently)</p></li>
</ul>
</li>
<li><p>Loop forever (for each episode):</p>
<ul>
<li><p>$b \leftarrow$ any soft policy</p></li>
<li><p>Generate an episode following $b$: $S_0, A_0, R_1, \ldots, S_{T-1}, A_{T-1}, R_T$</p></li>
<li><p>$G \leftarrow 0$</p></li>
<li><p>$W \leftarrow 1$</p></li>
<li><p>Loop for each step of episode, $t = T-1, T-2, \ldots, 0,$:</p>
<ul>
<li><p>$G \leftarrow \gamma G + R_{t+1}$</p></li>
<li><p>$C(S_t, A_t) \leftarrow C(S_t, A_t) + W$</p></li>
<li><p>$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{W}{C(S_t, A_t)} [G - Q(S_t, A_t)]$</p></li>
<li><p>$\pi(S_t) \leftarrow \arg \max_a Q(S_t,a)$ (with ties broken consistently)</p></li>
<li><p>If $A_t \neq \pi(S_t)$ then exit inner loop (proceed to next episode)</p></li>
<li><p>$W \leftarrow W \frac{1}{b(A_t|S_t)}$</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Notes:</p>
<ul>
<li><p>The policy $\pi$ converges to optimal at all encountered states even though actions are selected according to a different soft policy $b$,which may change between or even within episodes.</p></li>
<li><p>A potential problem is that this method learns only from the tails of episodes, when all of the remaining actions in the episode are greedy. If nongreedy actions are common, then learning will be slow, particularly for states appearing in the early portions of long episodes. Potentially, this could greatly slow learning.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section id="summary">
<h2>5.4 Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>Currently, Monte Carlo methods for both prediction and control remain unsettled and are a subject of ongoing research.</p>
<ul>
<li><p>Mindmap of where we are now</p>
  <img src="../img/chapter5/chapter5_mindmap.png" alt="Mindmap" style="width:100%;">
</li>
<li><p>Key Takeaways</p>
<ul class="simple">
<li><p>Advantages of MC over DP methods:</p>
<ol class="arabic simple">
<li><p>Monte Carlo methods require no model of the environment’s dynamics.</p></li>
<li><p>Monte Carlo methods can be used with simulation or sample models. For surprisingly many applications it is easy to simulate sample episodes even though it is diffcult to construct the kind of explicit model of transition probabilities.</p></li>
<li><p>It is easy and effcient to use Monte Carlo methods to focus on a small subset of the states. A region of special interest can be accurately evaluated without going to the expense of accurately evaluating the rest of the state set.</p></li>
<li><p>MC methods may be less harmed by violations of the Markov property. Because they do not update their value estimates on the basis of the value estimates of successor states, i.e., they do not bootstrap.</p></li>
</ol>
</li>
<li><p>Maintaining sufficient exploration:</p>
<ul>
<li><p>For on-policy methods: use assumption (1) of exploring start or initialize $\pi$ to be a $\epsilon$-soft policy</p></li>
<li><p>For off-policy methods: we learn the value function of a target policy from data generated by a different behavior policy, which satisfies the assumption of coverage. At the same time, we need importance sampling to transform the expected returns from the behavior policy to the target policy.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Extra lecture video (optional): <a class="reference external" href="https://www.coursera.org/learn/sample-based-learning-methods/lecture/mZvQp/emma-brunskill-batch-reinforcement-learning">Emma Brunskill: Batch Reinforcement Learning</a></p></li>
</ul>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="4_dynamic_programming.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 4. Dynamic Programming</p>
      </div>
    </a>
    <a class="right-next"
       href="6_temporal_difference_learning.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 6. Temporal-Difference Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-prediction-evaluation">5.1 Monte Carlo Prediction (Evaluation)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mc-prediction-for-state-value-function">5.1.1 MC Prediction for state-value function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mc-prediction-for-action-value-function">5.1.2 MC Prediction for action-value function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-control">5.2 Monte Carlo Control</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-control-removing-assumption-2">5.2.1 Monte Carlo Control removing Assumption (2)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-control-removing-both-assumptions">5.2.2 Monte Carlo Control removing both assumptions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#off-policy-monte-carlo-methods">5.3 Off-policy Monte Carlo Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#off-policy-monte-carlo-prediction-via-importance-sampling">5.3.1 Off-policy Monte Carlo Prediction via Importance Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#off-policy-monte-carlo-control">5.3.2 Off-policy Monte Carlo Control</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">5.4 Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Youxiang Dong
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025, Youxiang Dong.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>