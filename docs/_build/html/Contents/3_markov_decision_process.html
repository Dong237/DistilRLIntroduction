
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 3. Finite Markov Decision Processes &#8212; DistilRLIntro 0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=e1a75a79"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Contents/3_markov_decision_process';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 4. Dynamic Programming" href="4_dynamic_programming.html" />
    <link rel="prev" title="Chapte 2. Multi-armed Bandit" href="2_multi_armed_bandits.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">DistilRLIntro 0.1 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="0_prelude.html">Why this project</a></li>



<li class="toctree-l1"><a class="reference internal" href="1_intro.html">Chapter 1. Introduction to RL</a></li>
</ul>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2_multi_armed_bandits.html">Chapte 2. Multi-armed Bandit</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 3. Finite Markov Decision Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_dynamic_programming.html">Chapter 4. Dynamic Programming</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="5_monte_carlo_methods.html">Chapter 5. Monte Carlo Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="6_temporal_difference_learning.html">Chapter 6. Temporal-Difference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="7_planning_learning_acting.html">Chapter 7. Planning and Learning with Tabular Methods</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="8_on_policy_prediction_with_approximation.html">Chapter 8. On-policy Prediction with Approximation</a></li>
<li class="toctree-l1"><a class="reference internal" href="9_on_policy_control_with_approximation.html">Chapter 9. On-policy Control with Approximation</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_policy_gradient_methods.html">Chapter 10. Policy Gradient Methods</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Contents/3_markov_decision_process.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 3. Finite Markov Decision Processes</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#agent-environment-interface">3.1. Agent-Environment Interface</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#about-rewards-and-returns">3.2. About Rewards and Returns</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#goals-and-rewards">3.2.1 Goals and Rewards</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#returns-and-episodes">2.2 Returns and Episodes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unified-notation-for-episodic-and-continuing-tasks">3.2.3 Unified Notation for Episodic and Continuing Tasks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policies-and-value-functions">3.3. Policies and Value Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bellman-equations-optional-lecture-video">3.3.1 Bellman Equations (Optional Lecture video)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bellman-optimality-equation-optional-lecture-video">3.3.2 Bellman Optimality Equation (Optional Lecture video)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">3.4 Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-3-finite-markov-decision-processes">
<h1>Chapter 3. Finite Markov Decision Processes<a class="headerlink" href="#chapter-3-finite-markov-decision-processes" title="Link to this heading">#</a></h1>
<p>MDPs are a  formalization of sequential decision making, where actions influence both immediate rewards, and subsequent states, and thereby the future rewards. So it must consider the trade-off between the immediate reward and delayed reward.</p>
<p>Recall that in bandit problems we estimated the value <span class="math notranslate nohighlight">\(q_{\star}(a)\)</span> of each action <span class="math notranslate nohighlight">\(a\)</span>, in MDPs we would need to estimate the value <span class="math notranslate nohighlight">\(q_{\star}(s, a)\)</span> of each action <span class="math notranslate nohighlight">\(a\)</span> in each state <span class="math notranslate nohighlight">\(s\)</span>, or we estimate the value <span class="math notranslate nohighlight">\(v_{\star}(s)\)</span> of each state given optimal action selection. Meaning of these notations will be explained later in this chapter.</p>
<section id="agent-environment-interface">
<h2>3.1. Agent-Environment Interface<a class="headerlink" href="#agent-environment-interface" title="Link to this heading">#</a></h2>
<ul>
<li><p>Illustration of a MDP: MDPs are meant to be a straightforward framing of the problem of learning from interaction between a learner and the environment to achieve a goal, illustrated as follows:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter3/agent_env_interaction.png" alt="Agent Environment Interaction" style="width: 70%;">
  </div>
<ul class="simple">
<li><p>Explanation:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\textit{Agent}\)</span>: the learner and decision maker</p></li>
<li><p><span class="math notranslate nohighlight">\(\textit{Environment}\)</span>: the thing the agent interacts with, comprising everything outside the agent.</p></li>
<li><p>Interaction process: at time step <span class="math notranslate nohighlight">\(t\)</span>, the agent receives some representation of the environment’s state <span class="math notranslate nohighlight">\(S_t \in S\)</span>, selects on that basis an action <span class="math notranslate nohighlight">\(A_t \in A(s)\)</span>, as a consequence it then receives a numerical reward <span class="math notranslate nohighlight">\(R_{t+1} \in R \subset \mathbb{R}\)</span>, and finds itself in a new environment state <span class="math notranslate nohighlight">\(S_{t+1} \in S\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\textit{Trajectory}\)</span>: The sequence led by the interaction process: <span class="math notranslate nohighlight">\(s_0, a_0, r_1, s_1, a_1, r_2, ... , s_t, a_t, r_{t+1}\)</span> (sometimes when we talk about MDPs, we refer to this kind of sequences directly).</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Dynamics of MDP: in a <span class="math notranslate nohighlight">\(\textit{finite MDP}\)</span> - the sets of <span class="math notranslate nohighlight">\(S, A, R\)</span> all have finite elements, so <span class="math notranslate nohighlight">\(S_t, R_t\)</span> have well defined discrete probability distributions that are dependent only on the preceding state and action (<span class="math notranslate nohighlight">\(\textit{Markov property}\)</span>) - the dynamics of a finite MDP can be represented in this form as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
        p(s', r | s, a) \dot= Pr(S_{t+1}=s', R_{t+1}=r | A_t=a, S_t=s) \\
        \text{with} \sum_{s' \in S} \sum_{ r\in R} p(s', r | s, a) = 1, \text{for all} \ s \in S, a \in A(s)
    \end{split}\]</div>
</li>
<li><p>Useful derivations: with the dynamics of a MDP known, one can compute anything one might want to know about the envrionment:</p>
<ul>
<li><p>State-transition probability:</p>
<div class="math notranslate nohighlight">
\[
            p(s' | s, a) \doteq Pr(S_{t+1}=s' | A_t=a, S_t=s) = \sum_{r \in R}p(s', r | s, a)
        \]</div>
</li>
<li><p>Expected reward for state-action pairs:</p>
<div class="math notranslate nohighlight">
\[
            r(s,a) \doteq E[R_{t+1}|S_t=s, A_t=a] = \sum_{r \in R} r \sum_{s' \in S}p(s', r|s, a) 
        \]</div>
</li>
<li><p>Expected reward for state-action-next-state triples:</p>
<div class="math notranslate nohighlight">
\[
            r(s, a, s') \doteq \mathbb{E}[R_t \mid S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \frac{p(s', r \mid s, a)}{p(s' \mid s, a)}.
        \]</div>
</li>
</ul>
</li>
</ul>
</section>
<section id="about-rewards-and-returns">
<h2>3.2. About Rewards and Returns<a class="headerlink" href="#about-rewards-and-returns" title="Link to this heading">#</a></h2>
<section id="goals-and-rewards">
<h3>3.2.1 Goals and Rewards<a class="headerlink" href="#goals-and-rewards" title="Link to this heading">#</a></h3>
<ul>
<li><p>Definition of reward: In reinforcement learning, the purpose or goal of the agent is formalized in terms of a special scalar signal, called the <span class="math notranslate nohighlight">\(\textit{reward} \ (R_t \in R)\)</span>, passing from the environment to the agent.</p>
<p>Note that the reward signal is your way of communicating to the agent of what you want it to achieve, NOT how you want it achieved, i.e., reward signal doesn’t take the process into account.</p>
</li>
<li><p>Reward hypothesis: The idea of maximizing the cumulative reward to allow the agent to show desirablt behaviour is based on the <span class="math notranslate nohighlight">\(\textit{reward hypothesis}\)</span>: that all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of the reward.</p></li>
</ul>
</section>
<section id="returns-and-episodes">
<h3>2.2 Returns and Episodes<a class="headerlink" href="#returns-and-episodes" title="Link to this heading">#</a></h3>
<ul>
<li><p>Goal: In general, we seek to maximize the <span class="math notranslate nohighlight">\(\textit{expected return} \ G_t\)</span> of a sequence of rewards:</p>
<ul>
<li><p>for episodic tasks:</p>
<div class="math notranslate nohighlight">
\[
        G_t \dot= R_{t+1} + R_{t+2} + ... + R_{T} 
        \]</div>
</li>
<li><p>for continuing tasks:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
        \begin{align*}
        G_t \ &amp;\dot= \ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... \\
        &amp;= \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \\
        &amp;= R_{t+1} + \gamma \times G_{t+1}
        \end{align*}
        \end{split}\]</div>
</li>
</ul>
</li>
<li><p>Details on the two type of tasks:</p>
<ul>
<li><p>Episodic tasks: episodes end in a special state called the <span class="math notranslate nohighlight">\(\textit{terminal state}\)</span>, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. Note that</p>
<ul class="simple">
<li><p>the next episode begins independently of how the previous one ended</p></li>
<li><p>episodes can all be considered to end in the same <span class="math notranslate nohighlight">\(\textit{terminal state}\)</span></p></li>
<li><p>notation <span class="math notranslate nohighlight">\(S^+\)</span> is used to denote the set of all non-terminal states plus the terminal state.</p></li>
</ul>
</li>
<li><p>Continuing tasks: in contrast, continuing tasks are those tasks in which the agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit. Note that in continuous cases,</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\gamma \in (0,1)\)</span> is called the <span class="math notranslate nohighlight">\(\textit{discout rate}\)</span>, and is used to represent the agent’s preference between immediate and future reward. The more <span class="math notranslate nohighlight">\(\gamma\)</span> approaches 1, the more “farsighted” the agent becomes.</p></li>
<li><p>though <span class="math notranslate nohighlight">\(G_t\)</span> is a sum of an infinite number of terms, it is still finite if the reward is nonzero and constant and <span class="math notranslate nohighlight">\(\gamma \in (0,1)\)</span>.</p></li>
<li><p>special case for continuing tasks: if reward signal is +1 all the time, then:</p>
<div class="math notranslate nohighlight">
\[
            G_t = \sum_{k=0}^{\infty} \gamma^k = \frac{1}{1 - \gamma}
            \]</div>
</li>
<li><p>notation <span class="math notranslate nohighlight">\(S\)</span> is used to denote the set of all non-terminal states.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Example of the two type of tasks: Pole-Balancing (could be episode or continuing)</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter3/cart_pole.png" alt="Example of Cart-Pole" style="width: 50%;">
  </div>
<ul class="simple">
<li><p>Objective: to apply forces to a cart moving along a track so as to keep a pole hinged to the cart from falling over. The pole is reset to vertical after each failure.</p></li>
<li><p>Episodic perspective:</p>
<ul>
<li><p>Description: This task could be treated as episodic, where the natural episodes are the repeated attempts to balance the pole.</p></li>
<li><p>Reward: The reward in this case could be <span class="math notranslate nohighlight">\(+1\)</span> for every time step on which failure did not occur</p></li>
<li><p>Return: Return at each time would be the number of steps until failure.</p></li>
</ul>
</li>
<li><p>Continuing perspective:</p>
<ul>
<li><p>Description: We could also treat this as a continuing task, using discounting.</p></li>
<li><p>Reward: In this case the reward would be <span class="math notranslate nohighlight">\(-1\)</span> on each failure and zero at all other times.</p></li>
<li><p>Return: The return at each time step would then be related to <span class="math notranslate nohighlight">\(-\gamma^K\)</span>, where K is the number of time steps before failure. The return is maximized by keeping the pole balanced for as long as possible.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="unified-notation-for-episodic-and-continuing-tasks">
<h3>3.2.3 Unified Notation for Episodic and Continuing Tasks<a class="headerlink" href="#unified-notation-for-episodic-and-continuing-tasks" title="Link to this heading">#</a></h3>
<p>In practice, it turns out that when we discuss episodic tasks we almost never have to distinguish between different episodes. The two types of tasks can be unified by considering episode termination to be the entering of a special <span class="math notranslate nohighlight">\(\textit{absorbing state}\)</span> that transitions only to itself and that generates only rewards of zero.</p>
<div style="display: flex; justify-content: center;">
<img src="../_static/img/chapter3/absorbing_state.png" alt="Absorbing State" style="width: 60%;">
</div>
<p>So the expected return of both episodic and continuing tasks can now be written as <span class="math notranslate nohighlight">\(G_t=\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\)</span></p>
</section>
</section>
<section id="policies-and-value-functions">
<h2>3.3. Policies and Value Functions<a class="headerlink" href="#policies-and-value-functions" title="Link to this heading">#</a></h2>
<p>Almost all reinforcement learning algorithms involve estimating value functions - functions of states (or of state - action pairs) that estimate how good it is for the agent to be in a given state. We start this section by first defining policy:</p>
<ul>
<li><p>Definition: formally, a <span class="math notranslate nohighlight">\(\textit{policy}\)</span> is a mapping from states to probabilities of selecting each possible action:</p>
<div class="math notranslate nohighlight">
\[
        \pi(a|s) = Pr(A_t=a|S_t=s)
    \]</div>
</li>
</ul>
<p>and then we introduce Bellman equations and Bellman optimality equations for recursively computing value functions (both state-value and action-value functions).</p>
<section id="bellman-equations-optional-lecture-video">
<h3>3.3.1 Bellman Equations (<a class="reference external" href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/X5VDU/bellman-equation-derivation">Optional Lecture video</a>)<a class="headerlink" href="#bellman-equations-optional-lecture-video" title="Link to this heading">#</a></h3>
<ul>
<li><p>Value Function (of state <span class="math notranslate nohighlight">\(s\)</span>) under policy <span class="math notranslate nohighlight">\(\pi\)</span>: is the expected return when starting in state <span class="math notranslate nohighlight">\(s\)</span> and following <span class="math notranslate nohighlight">\(\pi\)</span> thereafter:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
        \begin{align*}
        v_{\pi}(s) \ &amp;\dot= \ \mathbb{E}_{\pi}[G_t|S_t=s] \\
        &amp;= \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1} | S_t=s\right] \\
        &amp;= \colorbox{lightyellow}{$\sum_a \pi(a|s)q(s,a)$} \\
        &amp;= \sum_a \pi(a|s) \sum_{s', r}p(s', r|s, a) [r + \gamma \mathbb{E}_{\pi}[G_{t+1}|S_{t+1}=s']] \\
        &amp;= \colorbox{lightyellow}{$\sum_a \pi(a|s) \sum_{s', r}p(s', r|s, a) [r + \gamma v_{\pi}(s')]$} \quad (\text{Bellman equation for} \ v_\pi)
        \end{align*}
    \end{split}\]</div>
<ul>
<li><p>States are the independent variables for the value function, i.e., <strong>for each input state, state-value function assigns a respective state value.</strong></p></li>
<li><p><strong>States-value functions are always defined by the policy</strong>, when changing the policy, the resulted state-value function will usually be different.</p></li>
<li><p>The final equation above is called <span class="math notranslate nohighlight">\(\textit{Bellman Equation}\)</span> for <span class="math notranslate nohighlight">\(v_{\pi}\)</span>, which expresses a relationship between the value of a state and the values of its successor states. The bellman equation can be understood with help of the following backup diagram for <span class="math notranslate nohighlight">\(v_{\pi}\)</span>:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter3/backup_diagram_v.png" alt="Backup diagram for v" style="width: 28%;">
  </div>
<p>The backup operations (from bottom <span class="math notranslate nohighlight">\(s'\)</span> to top <span class="math notranslate nohighlight">\(s\)</span>) transfer value information back to a state from its successor states.</p>
</li>
</ul>
</li>
<li><p>Action-value function under policy <span class="math notranslate nohighlight">\(\pi\)</span>: the expected return starting from <span class="math notranslate nohighlight">\(s\)</span>, taking the action <span class="math notranslate nohighlight">\(a\)</span>, and thereafter following policy <span class="math notranslate nohighlight">\(\pi\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
        \begin{align*}
        q_{\pi}(s,a) \ &amp;\dot= \ \mathbb{E}_{\pi}[G_t|S_t=s, A_t=a] \\
        &amp;= \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1} | S_t=s, A_t=a\right] \\
        &amp;= \colorbox{lightyellow}{$\sum_{s', r} p(s', r|s, a) (r + \gamma v(s'))$} \\
        &amp;= \sum_{s', r} p(s', r|s, a) (r + \gamma \sum_{a'} \pi(a'|s')\mathbb{E}_{\pi}[G_{t+1}|S_{t+1}=s', A_{t+1}=a']) \\
        &amp;= \colorbox{lightyellow}{$\sum_{s', r} p(s', r|s, a) [r+ \gamma \sum_{a'} \pi(a'|s') q(s', a')]$} \quad (\text{Bellman equation for} \ q_\pi)
        \end{align*}
    \end{split}\]</div>
<ul>
<li><p>Likewise, <span class="math notranslate nohighlight">\(q_{\pi}(s,a)\)</span> is a function of state <span class="math notranslate nohighlight">\(s\)</span> and action <span class="math notranslate nohighlight">\(a\)</span>, and is defined unique to the policy <span class="math notranslate nohighlight">\(\pi\)</span>.</p></li>
<li><p>The bellman equation for <span class="math notranslate nohighlight">\(q_{\pi}(s,a)\)</span> can be understood with help of the following backup diagram:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter3/backup_diagram_q.png" alt="Backup diagram for 1" style="width: 25%;">
  </div>
</li>
</ul>
</li>
<li><p>Example of Gridworld (lecture video)</p>
<p>Watch this lecture video linked to the following image, which gives a vivid example of how Bellman equation is computed in a gridworld environment. If the image is not clickable, try <a class="reference external" href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/in2Rn/why-bellman-equations">this link</a></p>
  <a href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/in2Rn/why-bellman-equations">
  <img src="../_static/img/chapter3/gridworld_example.png" alt="Video: Gridworld Example" style="width:70%;">
  </a>
</li>
</ul>
</section>
<section id="bellman-optimality-equation-optional-lecture-video">
<h3>3.3.2 Bellman Optimality Equation (<a class="reference external" href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/9DFPk/optimal-value-functions">Optional Lecture video</a>)<a class="headerlink" href="#bellman-optimality-equation-optional-lecture-video" title="Link to this heading">#</a></h3>
<ul>
<li><p>Optimal Policy: This <a class="reference external" href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/AjTR1/optimal-policies">lecture video</a> introduces optimal policy in depth if you find the textual definition too abstract.</p>
<ul class="simple">
<li><p>Better or Equal Policy: A policy <span class="math notranslate nohighlight">\(\pi\)</span> is defined to be <strong>better or equal</strong> to another policy <span class="math notranslate nohighlight">\(\pi'\)</span> if:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ v_{\pi}(s) \ge v_{\pi'}(s) \quad \text{for all } s \in S \]</div>
<ul class="simple">
<li><p>Optimal Policy (<span class="math notranslate nohighlight">\(\pi_{\star}\)</span>): An <strong>optimal policy</strong> <span class="math notranslate nohighlight">\(\pi_{\star}\)</span> is a policy that is better or equal to any other policy. Formally:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\pi_{\star}\)</span> must exist.</p></li>
<li><p>There can be more than one optimal policy.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Optimal value functions:</p>
<ul class="simple">
<li><p>Optimal State-Value Function: The <strong>optimal state-value function</strong> is defined as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ v_{\star}(s) \doteq \max_{\pi} v_{\pi}(s) \text{ for all } s \in S\]</div>
<ul class="simple">
<li><p>Optimal Action-Value Function: The <strong>optimal action-value function</strong> is defined as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ q_{\star}(s, a) \doteq \max_{\pi} q_{\pi}(s, a) \text{ for all } s \in S, a \in A(s) \]</div>
</li>
<li><p>Bellman Optimality Equation:</p>
<ul class="simple">
<li><p>for <span class="math notranslate nohighlight">\(v_{\star}(s)\)</span> (also written as <span class="math notranslate nohighlight">\(v_{\pi_\star}(s)\)</span>):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
	  v_{\star}(s) &amp;= \colorbox{lightyellow}{$ \underset{a \in A(s)}{\max} q_{\star}(s,a)$} \\
      &amp;= \underset{a}{\max} E_{\pi_{\star}}[R_{t+1} + \gamma G_{t+1} | S_t=s, A_t=a] \\
      &amp;= \underset{a}{\max} E_{\pi_{\star}}[R_{t+1} + \gamma v_{\star}(s') | S_t=s, A_t=a] \\
	  &amp;= \colorbox{lightyellow}{$ \underset{a}{\max} \sum_{s', r}p(s',r|s,a) (r+\gamma v_{\star}(s'))$}
    \end{align*}
    \end{split}\]</div>
<ul class="simple">
<li><p>for <span class="math notranslate nohighlight">\(q_{\star}(s,a)\)</span> (also written as <span class="math notranslate nohighlight">\(q_{\pi_\star}(s,a)\)</span>):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
	  q_{\star}(s,a) &amp;= E_{\pi_{\star}}[R_{t+1} + \gamma v_{\star}(s') | S_t=s, A_t=a] \\
      &amp;= \colorbox{lightyellow}{$ \underset{s', r}{\sum} p(s', r|s,a) [r+\gamma v_{\star}(s')] $}\\
	  &amp;= \colorbox{lightyellow}{$ \underset{s', r}{\sum} p(s', r|s,a) [r+ \gamma \ \max_a q_{\star}(s',a')]$}
    \end{align*}
    \end{split}\]</div>
<ul>
<li><p>Similarly, the bellman optimality equation can be easily memorized with help of these two backup diagrams:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../_static/img/chapter3/backup_diagam_optimality.png" alt="Backup diagram for $v_{\star}$ and $q_{\star}$" style="width: 70%;">
  </div>
<ul class="simple">
<li><p>Keep in mind that <strong><span class="math notranslate nohighlight">\(v_{\star}(s) =  \underset{a \in A(s)}{\max} q_{\star}(s,a)\)</span> is the key of deriving both bellmann optimality equations.</strong></p></li>
</ul>
</li>
</ul>
</li>
<li><p>Why are we doing: you could feel confused at this point of what are we doing here. Well in short, the one thing we need in the end of RL process is the (near-)optimal policy, which supports our (or should I say, the agent’s) decision making.</p>
<p>The optimal policy can be derived by having the optimal value function, and the optimal value function can be solved with bellmann optimality equation. To give you more details,</p>
<ul class="simple">
<li><p>the Bellman optimality equation is actually a system of equations, one for each state, so if there are <span class="math notranslate nohighlight">\(n\)</span> states, then there are <span class="math notranslate nohighlight">\(n\)</span> equations in <span class="math notranslate nohighlight">\(n\)</span> unknowns. <strong>If the dynamics <span class="math notranslate nohighlight">\(p\)</span> of the environment are known</strong>, then in principle one can solve this system of equations for <span class="math notranslate nohighlight">\(v_{\star}\)</span></p></li>
<li><p><strong>any policy that is greedy with respect to the optimal state-value function <span class="math notranslate nohighlight">\(v_{\star}\)</span> is an optimal policy</strong>, because <span class="math notranslate nohighlight">\(v_{\star}\)</span> already takes into account the reward consequences of all possible future behavior.</p></li>
</ul>
<p>In reality, optimal action-value function <span class="math notranslate nohighlight">\(q_{\star}\)</span> is often more desirable, with it, decisions can be made without knowing the dynamics of the environment. (<span class="math notranslate nohighlight">\(v_{\star}\)</span>, on the other hand, can only be used for decision making when environment dynamics are known - possible successor states and their values are known)</p>
</li>
</ul>
</section>
</section>
<section id="summary">
<h2>3.4 Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<ul>
<li><p>Key Takeaways:</p>
<ol class="arabic simple">
<li><p>MDP Basics:</p>
<ul class="simple">
<li><p>MDPs model sequential decision-making where actions influence immediate and future rewards.</p></li>
<li><p>The Markov property states that the next state depends only on the current state and action.</p></li>
</ul>
</li>
<li><p>Agent-Environment Interaction:</p>
<ul class="simple">
<li><p>The agent makes decisions; the environment provides state and reward feedback.</p></li>
<li><p>A trajectory is a sequence of states, actions, and rewards:  <span class="math notranslate nohighlight">\((s_0, a_0, r_1, s_1, a_1, r_2, \ldots)\)</span>.</p></li>
</ul>
</li>
<li><p>MDP Dynamics:</p>
<ul class="simple">
<li><p>Transition probability: <span class="math notranslate nohighlight">\(p(s', r | s, a) = \Pr(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)\)</span>.</p></li>
<li><p>Expected reward:  <span class="math notranslate nohighlight">\(r(s, a) = \mathbb{E}[R_{t+1} | S_t = s, A_t = a]\)</span>.</p></li>
</ul>
</li>
<li><p>Rewards and Returns:</p>
<ul class="simple">
<li><p>The reward signal defines what the agent should achieve, not how to achieve it.</p></li>
<li><p>The return <span class="math notranslate nohighlight">\(G_t\)</span> is the cumulative sum of rewards:</p>
<ul>
<li><p>Episodic tasks: <span class="math notranslate nohighlight">\(G_t = R_{t+1} + R_{t+2} + \ldots + R_T\)</span>.</p></li>
<li><p>Continuing tasks:  <span class="math notranslate nohighlight">\(G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\)</span>,  where <span class="math notranslate nohighlight">\(\gamma\)</span> is the discount factor <span class="math notranslate nohighlight">\((0 &lt; \gamma &lt; 1)\)</span>.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Policies and Value Functions:</p>
<ul class="simple">
<li><p>A policy <span class="math notranslate nohighlight">\(\pi\)</span> maps states to action probabilities: <span class="math notranslate nohighlight">\(\pi(a|s) = \Pr(A_t = a | S_t = s)\)</span>.</p></li>
<li><p>State-value function <span class="math notranslate nohighlight">\(v_\pi(s)\)</span>:  <span class="math notranslate nohighlight">\(v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]\)</span>.</p></li>
<li><p>Action-value function <span class="math notranslate nohighlight">\(q_\pi(s, a)\)</span>:  <span class="math notranslate nohighlight">\(q_\pi(s, a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a]\)</span>.</p></li>
</ul>
</li>
<li><p>Bellman (optimality) Equations: refer to <a class="reference internal" href="#policies-and-value-functions">section 3.3</a> for their derivations</p></li>
</ol>
</li>
<li><p>Final note:</p>
<p>Explicitly solving the Bellman optimality equation provides one route to finding an optimal policy, and thus to solving the reinforcement learning problem. However, this solution is rarely directly useful - it relies on at least three assumptions that are rarely true in practice:</p>
<ol class="arabic simple">
<li><p>we accurately know the dynamics <span class="math notranslate nohighlight">\(p\)</span> of the environment;</p></li>
<li><p>we have enough computational resources to complete the computation of the solution; and</p></li>
<li><p>the Markov property.</p></li>
</ol>
<p>Even in a simple tabular setting where 1. and 3. are met, the number of states could easily scale beyond any current suptercomputer’s ability. Therefore, there are other reinforcement learning methods that can be understood as <strong>approximately solving the Bellman optimality equation</strong>, and will be introduced in the following chapters.</p>
</li>
</ul>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="2_multi_armed_bandits.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapte 2. Multi-armed Bandit</p>
      </div>
    </a>
    <a class="right-next"
       href="4_dynamic_programming.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 4. Dynamic Programming</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#agent-environment-interface">3.1. Agent-Environment Interface</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#about-rewards-and-returns">3.2. About Rewards and Returns</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#goals-and-rewards">3.2.1 Goals and Rewards</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#returns-and-episodes">2.2 Returns and Episodes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unified-notation-for-episodic-and-continuing-tasks">3.2.3 Unified Notation for Episodic and Continuing Tasks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policies-and-value-functions">3.3. Policies and Value Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bellman-equations-optional-lecture-video">3.3.1 Bellman Equations (Optional Lecture video)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bellman-optimality-equation-optional-lecture-video">3.3.2 Bellman Optimality Equation (Optional Lecture video)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">3.4 Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Youxiang Dong
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025, Youxiang Dong.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>