
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 3. Finite Markov Decision Processes &#8212; DistilRLIntro 0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=e1a75a79"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Contents/3_markov_decision_process';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 4. Dynamic Programming" href="4_dynamic_programming.html" />
    <link rel="prev" title="Chapte 2. Multi-armed Bandit" href="2_multi_armed_bandits.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">DistilRLIntro 0.1 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="0_prelude.html">Why this project</a></li>



<li class="toctree-l1"><a class="reference internal" href="1_intro.html">Chapter 1. Introduction to RL</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Fundamentals of Reinforcement Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2_multi_armed_bandits.html">Chapte 2. Multi-armed Bandit</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 3. Finite Markov Decision Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_dynamic_programming.html">Chapter 4. Dynamic Programming</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Sample-based Learning Methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="5_monte_carlo_methods.html">Chapter 5. Monte Carlo Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="6_temporal_difference_learning.html">Chapter 6. Temporal-Difference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="7_planning_learning_acting.html">Chapter 7. Planning and Learning with Tabular Methods</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Value Function Approximation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="8_on_policy_prediction_with_approximation.html">Chapter 8. On-policy Prediction with Approximation</a></li>
<li class="toctree-l1"><a class="reference internal" href="9_on_policy_control_with_approximation.html">Chapter 9. On-policy Control with Approximation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Policy Approximation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10_policy_gradient_methods.html">Chapter 10. Policy Gradient Methods</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Contents/3_markov_decision_process.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 3. Finite Markov Decision Processes</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#agent-environment-interface">3.1. Agent-Environment Interface</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#about-rewards-and-returns">3.2. About Rewards and Returns</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#goals-and-rewards">3.2.1 Goals and Rewards</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#returns-and-episodes">2.2 Returns and Episodes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unified-notation-for-episodic-and-continuing-tasks">3.2.3 Unified Notation for Episodic and Continuing Tasks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policies-and-value-functions">3.3. Policies and Value Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bellman-equations-optional-lecture-video">3.3.1 Bellman Equations (Optional Lecture video)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bellman-optimality-equation-optional-lecture-video">3.3.2 Bellman Optimality Equation (Optional Lecture video)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">3.4 Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="chapter-3-finite-markov-decision-processes">
<h1>Chapter 3. Finite Markov Decision Processes<a class="headerlink" href="#chapter-3-finite-markov-decision-processes" title="Link to this heading">#</a></h1>
<p>MDPs are a  formalization of sequential decision making, where actions influence both immediate rewards, and subsequent states, and thereby the future rewards. So it must consider the trade-off between the immediate reward and delayed reward.</p>
<p>Recall that in bandit problems we estimated the value $q_{\star}(a)$ of each action $a$, in MDPs we would need to estimate the value $q_{\star}(s, a)$ of each action $a$ in each state $s$, or we estimate the value $v_{\star}(s)$ of each state given optimal action selection. Meaning of these notations will be explained later in this chapter.</p>
<section id="agent-environment-interface">
<h2>3.1. Agent-Environment Interface<a class="headerlink" href="#agent-environment-interface" title="Link to this heading">#</a></h2>
<ul>
<li><p>Illustration of a MDP: MDPs are meant to be a straightforward framing of the problem of learning from interaction between a learner and the environment to achieve a goal, illustrated as follows:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../img/chapter3/agent_env_interaction.png" alt="Agent Environment Interaction" style="width: 70%;">
  </div>
<ul class="simple">
<li><p>Explanation:</p>
<ul>
<li><p>$\textit{Agent}$: the learner and decision maker</p></li>
<li><p>$\textit{Environment}$: the thing the agent interacts with, comprising everything outside the agent.</p></li>
<li><p>Interaction process: at time step $t$, the agent receives some representation of the environment’s state $S_t \in S$, selects on that basis an action $A_t \in A(s)$, as a consequence it then receives a numerical reward $R_{t+1} \in R \subset \mathbb{R}$, and finds itself in a new environment state $S_{t+1} \in S$.</p></li>
<li><p>$\textit{Trajectory}$: The sequence led by the interaction process: $s_0, a_0, r_1, s_1, a_1, r_2, … , s_t, a_t, r_{t+1}$ (sometimes when we talk about MDPs, we refer to this kind of sequences directly).</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Dynamics of MDP: in a $\textit{finite MDP}$ - the sets of $S, A, R$ all have finite elements, so $S_t, R_t$ have well defined discrete probability distributions that are dependent only on the preceding state and action ($\textit{Markov property}$) - the dynamics of a finite MDP can be represented in this form as follows:</p>
<p>$$
p(s\prime, r | s, a) \dot= Pr(S_{t+1}=s\prime, R_{t+1}=r | A_t=a, S_t=s) \
\text{with} \sum_{s\prime \in S} \sum_{ r\in R} p(s\prime, r | s, a) = 1, \text{for all} \ s \in S, a \in A(s)
$$</p>
</li>
<li><p>Useful derivations: with the dynamics of a MDP known, one can compute anything one might want to know about the envrionment:</p>
<ul>
<li><p>State-transition probability:
$$
p(s\prime | s, a) \doteq Pr(S_{t+1}=s\prime | A_t=a, S_t=s) = \sum_{r \in R}p(s\prime, r | s, a)
$$</p></li>
<li><p>Expected reward for state-action pairs:
$$
r(s,a) \doteq E[R_{t+1}|S_t=s, A_t=a] = \sum_{r \in R} r \sum_{s\prime \in S}p(s\prime, r|s, a)
$$</p></li>
<li><p>Expected reward for state-action-next-state triples:</p>
<p>$$
r(s, a, s’) \doteq \mathbb{E}[R_t \mid S_{t-1} = s, A_{t-1} = a, S_t = s’] = \sum_{r \in \mathcal{R}} r \frac{p(s’, r \mid s, a)}{p(s’ \mid s, a)}.
$$</p>
</li>
</ul>
</li>
</ul>
</section>
<section id="about-rewards-and-returns">
<h2>3.2. About Rewards and Returns<a class="headerlink" href="#about-rewards-and-returns" title="Link to this heading">#</a></h2>
<section id="goals-and-rewards">
<h3>3.2.1 Goals and Rewards<a class="headerlink" href="#goals-and-rewards" title="Link to this heading">#</a></h3>
<ul>
<li><p>Definition of reward: In reinforcement learning, the purpose or goal of the agent is formalized in terms of a special scalar signal, called the $\textit{reward} \ (R_t \in R)$, passing from the environment to the agent.</p>
<p>Note that the reward signal is your way of communicating to the agent of what you want it to achieve, NOT how you want it achieved, i.e., reward signal doesn’t take the process into account.</p>
</li>
<li><p>Reward hypothesis: The idea of maximizing the cumulative reward to allow the agent to show desirablt behaviour is based on the $\textit{reward hypothesis}$: that all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of the reward.</p></li>
</ul>
</section>
<section id="returns-and-episodes">
<h3>2.2 Returns and Episodes<a class="headerlink" href="#returns-and-episodes" title="Link to this heading">#</a></h3>
<ul>
<li><p>Goal: In general, we seek to maximize the $\textit{expected return} \ G_t$ of a sequence of rewards:</p>
<ul class="simple">
<li><p>for episodic tasks:
$$
G_t \dot= R_{t+1} + R_{t+2} + … + R_{T}
$$</p></li>
<li><p>for continuing tasks:
$$
\begin{align*}
G_t \ &amp;\dot= \ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + … \
&amp;= \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \
&amp;= R_{t+1} + \gamma \times G_{t+1}
\end{align*}
$$</p></li>
</ul>
</li>
<li><p>Details on the two type of tasks:</p>
<ul class="simple">
<li><p>Episodic tasks: episodes end in a special state called the $\textit{terminal state}$, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. Note that</p>
<ul>
<li><p>the next episode begins independently of how the previous one ended</p></li>
<li><p>episodes can all be considered to end in the same $\textit{terminal state}$</p></li>
<li><p>notation $S^+$ is used to denote the set of all non-terminal states plus the terminal state.</p></li>
</ul>
</li>
<li><p>Continuing tasks: in contrast, continuing tasks are those tasks in which the agent–environment interaction does not break naturally into identifiable episodes, but goes on continually without limit. Note that in continuous cases,</p>
<ul>
<li><p>$\gamma \in (0,1)$ is called the $\textit{discout rate}$, and is used to represent the agent’s preference between immediate and future reward. The more $\gamma$ approaches 1, the more “farsighted” the agent becomes.</p></li>
<li><p>though $G_t$ is a sum of an infinite number of terms, it is still finite if the reward is nonzero and constant and $\gamma \in (0,1)$.</p></li>
<li><p>special case for continuing tasks: if reward signal is +1 all the time, then:
$$
G_t = \sum_{k=0}^{\infty} \gamma^k = \frac{1}{1 - \gamma}
$$</p></li>
<li><p>notation $S$ is used to denote the set of all non-terminal states.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Example of the two type of tasks: Pole-Balancing (could be episode or continuing)</p>
  <div style="display: flex; justify-content: center;">
  <img src="../img/chapter3/cart_pole.png" alt="Example of Cart-Pole" style="width: 50%;">
  </div>
<ul class="simple">
<li><p>Objective: to apply forces to a cart moving along a track so as to keep a pole hinged to the cart from falling over. The pole is reset to vertical after each failure.</p></li>
<li><p>Episodic perspective:</p>
<ul>
<li><p>Description: This task could be treated as episodic, where the natural episodes are the repeated attempts to balance the pole.</p></li>
<li><p>Reward: The reward in this case could be $+1$ for every time step on which failure did not occur</p></li>
<li><p>Return: Return at each time would be the number of steps until failure.</p></li>
</ul>
</li>
<li><p>Continuing perspective:</p>
<ul>
<li><p>Description: We could also treat this as a continuing task, using discounting.</p></li>
<li><p>Reward: In this case the reward would be $-1$ on each failure and zero at all other times.</p></li>
<li><p>Return: The return at each time step would then be related to $-\gamma^K$, where K is the number of time steps before failure. The return is maximized by keeping the pole balanced for as long as possible.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="unified-notation-for-episodic-and-continuing-tasks">
<h3>3.2.3 Unified Notation for Episodic and Continuing Tasks<a class="headerlink" href="#unified-notation-for-episodic-and-continuing-tasks" title="Link to this heading">#</a></h3>
<p>In practice, it turns out that when we discuss episodic tasks we almost never have to distinguish between different episodes. The two types of tasks can be unified by considering episode termination to be the entering of a special $\textit{absorbing state}$ that transitions only to itself and that generates only rewards of zero.</p>
<div style="display: flex; justify-content: center;">
<img src="../img/chapter3/absorbing_state.png" alt="Absorbing State" style="width: 60%;">
</div>
<p>So the expected return of both episodic and continuing tasks can now be written as $G_t=\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$</p>
</section>
</section>
<section id="policies-and-value-functions">
<h2>3.3. Policies and Value Functions<a class="headerlink" href="#policies-and-value-functions" title="Link to this heading">#</a></h2>
<p>Almost all reinforcement learning algorithms involve estimating value functions - functions of states (or of state - action pairs) that estimate how good it is for the agent to be in a given state. We start this section by first defining policy:</p>
<ul>
<li><p>Definition: formally, a $\textit{policy}$ is a mapping from states to probabilities of selecting each possible action:</p>
<p>$$
\pi(a|s) = Pr(A_t=a|S_t=s)
$$</p>
</li>
</ul>
<p>and then we introduce Bellman equations and Bellman optimality equations for recursively computing value functions (both state-value and action-value functions).</p>
<section id="bellman-equations-optional-lecture-video">
<h3>3.3.1 Bellman Equations (<a class="reference external" href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/X5VDU/bellman-equation-derivation">Optional Lecture video</a>)<a class="headerlink" href="#bellman-equations-optional-lecture-video" title="Link to this heading">#</a></h3>
<ul>
<li><p>Value Function (of state $s$) under policy $\pi$: is the expected return when starting in state $s$ and following $\pi$ thereafter:</p>
<p>$$
\begin{align*}
v_{\pi}(s) \ &amp;\dot= \ \mathbb{E}<em>{\pi}[G_t|S_t=s] \
&amp;= \mathbb{E}</em>{\pi}\left[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1} | S_t=s\right] \
&amp;= \colorbox{lightyellow}{$\sum_a \pi(a|s)q(s,a)$} \
&amp;= \sum_a \pi(a|s) \sum_{s’, r}p(s’, r|s, a) [r + \gamma \mathbb{E}<em>{\pi}[G</em>{t+1}|S_{t+1}=s’]] \
&amp;= \colorbox{lightyellow}{$\sum_a \pi(a|s) \sum_{s’, r}p(s’, r|s, a) [r + \gamma v_{\pi}(s’)]$} \quad (\text{Bellman equation for} \ v_\pi)
\end{align*}
$$</p>
<ul>
<li><p>States are the independent variables for the value function, i.e., <strong>for each input state, state-value function assigns a respective state value.</strong></p></li>
<li><p><strong>States-value functions are always defined by the policy</strong>, when changing the policy, the resulted state-value function will usually be different.</p></li>
<li><p>The final equation above is called $\textit{Bellman Equation}$ for $v_{\pi}$, which expresses a relationship between the value of a state and the values of its successor states. The bellman equation can be understood with help of the following backup diagram for $v_{\pi}$:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../img/chapter3/backup_diagram_v.png" alt="Backup diagram for v" style="width: 28%;">
  </div>
<p>The backup operations (from bottom $s’$ to top $s$) transfer value information back to a state from its successor states.</p>
</li>
</ul>
</li>
<li><p>Action-value function under policy $\pi$: the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$:</p>
<p>$$
\begin{align*}
q_{\pi}(s,a) \ &amp;\dot= \ \mathbb{E}<em>{\pi}[G_t|S_t=s, A_t=a] \
&amp;= \mathbb{E}</em>{\pi}\left[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1} | S_t=s, A_t=a\right] \
&amp;= \colorbox{lightyellow}{$\sum_{s’, r} p(s’, r|s, a) (r + \gamma v(s’))$}\
&amp;= \sum_{s’, r} p(s’, r|s, a) (r + \gamma \sum_{a’} \pi(a’|s’)\mathbb{E}<em>{\pi}[G</em>{t+1}|S_{t+1}=s’, A_{t+1}=a’]) \
&amp;= \colorbox{lightyellow}{$\sum_{s’, r} p(s’, r|s, a) [r+ \gamma \sum_{a’} \pi(a’|s’) q(s’, a’)]$} \quad (\text{Bellman equation for} \ q_\pi)
\end{align*}
$$</p>
<ul>
<li><p>Likewise, $q_{\pi}(s,a)$ is a function of state $s$ and action $a$, and is defined unique to the policy $\pi$.</p></li>
<li><p>The bellman equation for $q_{\pi}(s,a)$ can be understood with help of the following backup diagram:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../img/chapter3/backup_diagram_q.png" alt="Backup diagram for 1" style="width: 25%;">
  </div>
</li>
</ul>
</li>
<li><p>Example of Gridworld (lecture video)</p>
<p>Watch this lecture video linked to the following image, which gives a vivid example of how Bellman equation is computed in a gridworld environment. If the image is not clickable, try <a class="reference external" href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/in2Rn/why-bellman-equations">this link</a></p>
  <a href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/in2Rn/why-bellman-equations">
  <img src="../img/chapter3/gridworld_example.png" alt="Video: Gridworld Example" style="width:70%;">
  </a>
</li>
</ul>
</section>
<section id="bellman-optimality-equation-optional-lecture-video">
<h3>3.3.2 Bellman Optimality Equation (<a class="reference external" href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/9DFPk/optimal-value-functions">Optional Lecture video</a>)<a class="headerlink" href="#bellman-optimality-equation-optional-lecture-video" title="Link to this heading">#</a></h3>
<ul>
<li><p>Optimal Policy: This <a class="reference external" href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/AjTR1/optimal-policies">lecture video</a> introduces optimal policy in depth if you find the textual definition too abstract.</p>
<ul class="simple">
<li><p>Better or Equal Policy: A policy $\pi$ is defined to be <strong>better or equal</strong> to another policy $\pi’$ if:
$$ v_{\pi}(s) \ge v_{\pi’}(s) \quad \text{for all } s \in S $$</p></li>
<li><p>Optimal Policy ($\pi_{\star}$): An <strong>optimal policy</strong> $\pi_{\star}$ is a policy that is better or equal to any other policy. Formally:</p>
<ul>
<li><p>$\pi_{\star}$ must exist.</p></li>
<li><p>There can be more than one optimal policy.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Optimal value functions:</p>
<ul class="simple">
<li><p>Optimal State-Value Function: The <strong>optimal state-value function</strong> is defined as:
$$ v_{\star}(s) \doteq \max_{\pi} v_{\pi}(s) \text{ for all } s \in S$$</p></li>
<li><p>Optimal Action-Value Function: The <strong>optimal action-value function</strong> is defined as:
$$ q_{\star}(s, a) \doteq \max_{\pi} q_{\pi}(s, a) \text{ for all } s \in S, a \in A(s) $$</p></li>
</ul>
</li>
<li><p>Bellman Optimality Equation:</p>
<ul class="simple">
<li><p>for $v_{\star}(s)$ (also written as $v_{\pi_\star}(s)$):</p></li>
</ul>
<p>$$
\begin{align*}
v_{\star}(s) &amp;= \colorbox{lightyellow}{$ \underset{a \in A(s)}{\max} q_{\star}(s,a)$} \
&amp;= \underset{a}{\max} E_{\pi_{\star}}[R_{t+1} + \gamma G_{t+1} | S_t=s, A_t=a] \
&amp;= \underset{a}{\max} E_{\pi_{\star}}[R_{t+1} + \gamma v_{\star}(s\prime) | S_t=s, A_t=a] \
&amp;= \colorbox{lightyellow}{$ \underset{a}{\max} \sum_{s\prime, r}p(s\prime,r|s,a) (r+\gamma v_{\star}(s\prime))$}
\end{align*}
$$</p>
<ul class="simple">
<li><p>for $q_{\star}(s,a)$ (also written as $q_{\pi_\star}(s,a)$):</p></li>
</ul>
<p>$$
\begin{align*}
q_{\star}(s,a) &amp;= E_{\pi_{\star}}[R_{t+1} + \gamma v_{\star}(s\prime) | S_t=s, A_t=a] \
&amp;= \colorbox{lightyellow}{$ \underset{s\prime, r}{\sum} p(s\prime, r|s,a) [r+\gamma v_{\star}(s\prime)] $}\
&amp;= \colorbox{lightyellow}{$ \underset{s\prime, r}{\sum} p(s\prime, r|s,a) [r+ \gamma \ \max_a q_{\star}(s\prime,a\prime)]$}
\end{align*}
$$</p>
<ul>
<li><p>Similarly, the bellman optimality equation can be easily memorized with help of these two backup diagrams:</p>
  <div style="display: flex; justify-content: center;">
  <img src="../img/chapter3/backup_diagam_optimality.png" alt="Backup diagram for $v_{\star}$ and $q_{\star}$" style="width: 70%;">
  </div>
<ul class="simple">
<li><p>Keep in mind that <strong>$v_{\star}(s) =  \underset{a \in A(s)}{\max} q_{\star}(s,a)$ is the key of deriving both bellmann optimality equations.</strong></p></li>
</ul>
</li>
</ul>
</li>
<li><p>Why are we doing: you could feel confused at this point of what are we doing here. Well in short, the one thing we need in the end of RL process is the (near-)optimal policy, which supports our (or should I say, the agent’s) decision making.</p>
<p>The optimal policy can be derived by having the optimal value function, and the optimal value function can be solved with bellmann optimality equation. To give you more details,</p>
<ul class="simple">
<li><p>the Bellman optimality equation is actually a system of equations, one for each state, so if there are $n$ states, then there are $n$ equations in $n$ unknowns. <strong>If the dynamics $p$ of the environment are known</strong>, then in principle one can solve this system of equations for $v_{\star}$</p></li>
<li><p><strong>any policy that is greedy with respect to the optimal state-value function $v_{\star}$ is an optimal policy</strong>, because $v_{\star}$ already takes into account the reward consequences of all possible future behavior.</p></li>
</ul>
<p>In reality, optimal action-value function $q_{\star}$ is often more desirable, with it, decisions can be made without knowing the dynamics of the environment. ($v_{\star}$, on the other hand, can only be used for decision making when environment dynamics are known - possible successor states and their values are known)</p>
</li>
</ul>
</section>
</section>
<section id="summary">
<h2>3.4 Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<ul>
<li><p>Key Takeaways:</p>
<ol class="arabic simple">
<li><p>MDP Basics:</p>
<ul class="simple">
<li><p>MDPs model sequential decision-making where actions influence immediate and future rewards.</p></li>
<li><p>The Markov property states that the next state depends only on the current state and action.</p></li>
</ul>
</li>
<li><p>Agent-Environment Interaction:</p>
<ul class="simple">
<li><p>The agent makes decisions; the environment provides state and reward feedback.</p></li>
<li><p>A trajectory is a sequence of states, actions, and rewards:  $(s_0, a_0, r_1, s_1, a_1, r_2, \ldots)$.</p></li>
</ul>
</li>
<li><p>MDP Dynamics:</p>
<ul class="simple">
<li><p>Transition probability:      $p(s’, r | s, a) = \Pr(S_{t+1} = s’, R_{t+1} = r | S_t = s, A_t = a)$.</p></li>
<li><p>Expected reward:  $r(s, a) = \mathbb{E}[R_{t+1} | S_t = s, A_t = a]$.</p></li>
</ul>
</li>
<li><p>Rewards and Returns:</p>
<ul class="simple">
<li><p>The reward signal defines what the agent should achieve, not how to achieve it.</p></li>
<li><p>The return $G_t$ is the cumulative sum of rewards:</p>
<ul>
<li><p>Episodic tasks: $G_t = R_{t+1} + R_{t+2} + \ldots + R_T$.</p></li>
<li><p>Continuing tasks:  $G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$,  where $\gamma$ is the discount factor $(0 &lt; \gamma &lt; 1)$.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Policies and Value Functions:</p>
<ul class="simple">
<li><p>A policy $\pi$ maps states to action probabilities: $\pi(a|s) = \Pr(A_t = a | S_t = s)$.</p></li>
<li><p>State-value function $v_\pi(s)$:  $v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]$.</p></li>
<li><p>Action-value function $q_\pi(s, a)$:  $q_\pi(s, a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a]$.</p></li>
</ul>
</li>
<li><p>Bellman (optimality) Equations: refer to <a class="reference internal" href="#33-policies-and-value-functions"><span class="xref myst">section 3.3</span></a> for their derivations</p></li>
</ol>
</li>
<li><p>Final note:</p>
<p>Explicitly solving the Bellman optimality equation provides one route to finding an optimal policy, and thus to solving the reinforcement learning problem. However, this solution is rarely directly useful - it relies on at least three assumptions that are rarely true in practice:</p>
<ol class="arabic simple">
<li><p>we accurately know the dynamics $p$ of the environment;</p></li>
<li><p>we have enough computational resources to complete the computation of the solution; and</p></li>
<li><p>the Markov property.</p></li>
</ol>
<p>Even in a simple tabular setting where 1. and 3. are met, the number of states could easily scale beyond any current suptercomputer’s ability. Therefore, there are other reinforcement learning methods that can be understood as <strong>approximately solving the Bellman optimality equation</strong>, and will be introduced in the following chapters.</p>
</li>
</ul>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="2_multi_armed_bandits.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapte 2. Multi-armed Bandit</p>
      </div>
    </a>
    <a class="right-next"
       href="4_dynamic_programming.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 4. Dynamic Programming</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#agent-environment-interface">3.1. Agent-Environment Interface</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#about-rewards-and-returns">3.2. About Rewards and Returns</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#goals-and-rewards">3.2.1 Goals and Rewards</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#returns-and-episodes">2.2 Returns and Episodes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unified-notation-for-episodic-and-continuing-tasks">3.2.3 Unified Notation for Episodic and Continuing Tasks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#policies-and-value-functions">3.3. Policies and Value Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bellman-equations-optional-lecture-video">3.3.1 Bellman Equations (Optional Lecture video)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bellman-optimality-equation-optional-lecture-video">3.3.2 Bellman Optimality Equation (Optional Lecture video)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">3.4 Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Youxiang Dong
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025, Youxiang Dong.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>